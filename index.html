<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Investigations into hardware and software details">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Journey to the Center of the Computer | Journey to the Center of the Computer</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="https://markdewing.github.io/blog/">
<link rel="next" href="index-1.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><link rel="prefetch" href="posts/integration-callbacks/" type="text/html">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://markdewing.github.io/blog/">

                <span id="blog-title">Journey to the Center of the Computer</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>
                </li>
<li>
<a href="rss.xml">RSS feed</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

<div class="postindex">
    <article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/integration-callbacks/" class="u-url">Integration Callbacks with Sympy and LLVM</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">Mark Dewing</span></p>
            <p class="dateline"><a href="posts/integration-callbacks/" rel="bookmark"><time class="published dt-published" datetime="2016-07-08T16:37:00-05:00" title="2016-07-08 16:37">2016-07-08 16:37</time></a></p>
                <p class="commentline">
        
    <a href="posts/integration-callbacks/#disqus_thread" data-disqus-identifier="cache/posts/integration_callbacks.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>This post explores various packages for multi-dimensional integration along with
generating callbacks for the integrands from Sympy using an LLVM JIT</p>
<h3>Problem to integrate</h3>
<p>The particular problem is using the variational principle to find the ground state energy for atoms.
Some Jupyter notebooks with a description of the problem, along with various integration methods:</p>
<p><a href="https://github.com/markdewing/next_steps_in_programming/blob/master/examples/integration/Hydogen%20Atom.ipynb">Ground state energy of Hydrogen Atom</a>   (This yields a 3 dimensional integral.)</p>
<p><a href="https://github.com/markdewing/next_steps_in_programming/blob/master/examples/integration/Helium%20atom.ipynb">Ground state energy of Helium Atom</a>  (This yields a 6 dimensional integral.)</p>
<p>The standard solution to these integrals is to use Markov Chain Monte Carlo (the Quantum Monte Carlo method).<br>
However, I'm curious to see how far alternate integration schemes or existing integration packages would work.</p>
<h3>Integration libraries</h3>
<p>The <a href="http://docs.scipy.org/doc/scipy/reference/tutorial/integrate.html">scipy quadrature</a> routines accept a natively compiled callback for the integrand. 
(Noticing this in the documentation initiated the idea for using JIT compilation for callback functions.)</p>
<p>Next up is the <a href="http://ab-initio.mit.edu/wiki/index.php/Cubature">Cubature</a> integration package, with the <a href="https://github.com/saullocastro/cubature">Python wrapper for cubature</a></p>
<p>Finally is the <a href="http://www.feynarts.de/cuba/">Cuba</a> library, with the PyCuba interface (part of the <a href="https://github.com/JohannesBuchner/PyMultiNest">PyMultiNest</a> package)</p>
<p>There are some other libraries such at <a href="http://mint.sbg.ac.at/HIntLib/">HIntLib</a> that I would also like to try.  There doesn't seem to be a python interface for HIntLib.  Let me know if there is one somewhere. And if there are other multidimensional integration packages to try.</p>
<h3>Evaluating the integrand</h3>
<p>One of my scientific programming goals is to generate efficient code from a symbolic expression.
To this end, I've been working on an LLVM JIT converter for Sympy expressions (using the <a href="https://github.com/numba/llvmlite">llvmlite</a> wrapper).</p>
<p>For the Sympy code, see these pull requests: </p>
<ul>
<li>
<a href="https://github.com/sympy/sympy/pull/10451">Create executable functions from Sympy expressions</a> </li>
<li><a href="https://github.com/sympy/sympy/pull/10640">Accelerated callbacks for integration routines</a></li>
<li><a href="https://github.com/sympy/sympy/pull/10683">JIT - handle multiple expressions (as returned from CSE)</a></li>
<li><a href="https://github.com/sympy/sympy/pull/11057">Add LLVM JIT callbacks for PyCuba integration</a></li>
</ul>
<p>As an aside, one can question if is this the right approach, compared with</p>
<ol>
<li>Generate C++ or Fortran and compile using the existing autowrap functionality in Sympy.</li>
<li>Generate Python/Numpy and use Numba.</li>
<li>Use Julia</li>
</ol>
<p>There is always a tradeoff between a narrow, specialized solution, which is faster to implement and
perhaps easier to understand, and a more general solution, which applies in more cases, but is
harder and slower to implement.</p>
<p>Using an LLVM JIT is a specialized solution, but it does have an advantage that there is a short path from the expressions to the compiled code.
One disadvantage is that it does not leverage existing compilers (Numba or C++), though LLVM compiler optimization passes are available.</p>
<p>Sometimes a solution just needs to be tried to gain experience with the advantages and drawbacks.</p>
<h3>Results</h3>
<p>For the helium atom, the integration times are reported in the table below</p>
<table>
<thead><tr>
<th align="left">Integrator  </th>
<th align="right">Time (seconds)</th>
</tr></thead>
<tbody>
<tr>
<td align="left">Cubature</td>
<td align="right">171</td>
</tr>
<tr>
<td align="left">Cubature w/CSE</td>
<td align="right">141</td>
</tr>
<tr>
<td align="left">Cubature w/CSE and multiple evals</td>
<td align="right">100</td>
</tr>
<tr>
<td align="left">Cuba (Vegas)</td>
<td align="right">29</td>
</tr>
<tr>
<td align="left">Cuba (Cuhre)</td>
<td align="right">22</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>Note that <code>scipy.nquad</code> was not used for the 6D integral. It quickly runs out of steam because it consists of iterated one dimensional integrations, and the glue between the dimensions goes through Python, reducing the effectiveness of a compiled integrand.</p>
<p>The Cubature library does better.  Profiling shows that most of the time is spent internal to cubature and allocating memory, so faster integrand evaluation is not going to improve the time.
Some other approaches can help.  One is Common Subexpression Elimination (CSE), which Sympy can perform on the expression.  This extracts duplicate fragments so their value only needs to be computed once.</p>
<p>The library also allows multiple integrals to be performed at once.   This can amortize some of the overhead of the library.  In this case, the individual calls to integrator for the numerator and denominator can be reduced to a single call.</p>
<p>The Cuba library performs even better, as there is apparently less overhead inside the integration library.  The Cuhre integrator uses a deterministic grid-based algorithm similar to Cubature.  Vegas uses an adaptive Monte Carlo approach.</p>
<p>The results are not shown here, but I also used SIMD vectorization to make the function evaluation even faster, which succeeded for the bare function evaluation. (This was one of the original motivations for compiling straight to LLVM, as it would be easier to get vectorization working.)
 Unfortunately, it did not speed up the overall integration much (if at all), due to overhead in the libraries.</p>
<h3>Conclusions and future work</h3>
<p>Using an LLVM JIT to create callbacks for integration works fairly well.</p>
<p>One important question is how to scale the creation of the callbacks to new libraries without explicitly programming them into Sympy.<br>
The <a href="https://github.com/sympy/sympy/pull/11057">last pull request</a> has expanded the <code>CodeSignature</code> class, which seems like  a starting point for such a more general callback specification.</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/notes-on-cmake/" class="u-url">Notes on CMake</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">Mark Dewing</span></p>
            <p class="dateline"><a href="posts/notes-on-cmake/" rel="bookmark"><time class="published dt-published" datetime="2016-02-19T05:05:00-06:00" title="2016-02-19 05:05">2016-02-19 05:05</time></a></p>
                <p class="commentline">
        
    <a href="posts/notes-on-cmake/#disqus_thread" data-disqus-identifier="cache/posts/notes-on-cmake.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>Recently I started working on a project that uses CMake.  I've used CMake a little before, but never really
had to dive much into it.
In particular, I needed to understand the scripting parts of CMake for adding tests for CTest.</p>
<p>Below are some comments on aspects of CMake.</p>
<h3>Variables and variable substitution</h3>
<p>Variables names are strings.  Substitution occurs when the variable is dereferenced with <code>${}</code>.</p>
<pre class="code literal-block"><span class="nb">SET</span><span class="p">(</span><span class="s">var,</span> <span class="s">a</span><span class="p">)</span>
<span class="nb">MESSAGE</span><span class="p">(</span><span class="s2">"var = ${var}"</span><span class="p">)</span>
</pre>


<p>produces</p>
<pre class="code literal-block"><span class="n">var</span> <span class="o">=</span> <span class="n">a</span>
</pre>


<p>Nested substitutions are possible</p>
<pre class="code literal-block"><span class="nb">SET</span><span class="p">(</span><span class="s">var,</span> <span class="s">a</span><span class="p">)</span>
<span class="nb">SET</span><span class="p">(</span><span class="s">a,</span> <span class="s">b</span><span class="p">)</span>
<span class="nb">MESSAGE</span><span class="p">(</span><span class="s2">"var = ${var}  ${${var}}"</span><span class="p">)</span>
</pre>


<p>will produce 
<code>var = a b</code></p>
<p>Variable names can be composed during substitution</p>
<pre class="code literal-block"><span class="nb">SET</span><span class="p">(</span><span class="s">var,</span> <span class="s">a</span><span class="p">)</span>
<span class="nb">SET</span><span class="p">(</span><span class="s">a_one,</span> <span class="s">apple</span><span class="p">)</span>
<span class="nb">MESSAGE</span><span class="p">(</span><span class="s2">"var =  ${${var}_one}"</span><span class="p">)</span>
</pre>


<p>will produce <code>var = apple</code></p>
<h3>Variables and functions</h3>
<p>Variable references act a little like pointers, but without a type system to enforce (and guide) how many indirections should be performed.</p>
<p>Example of using a variable inside a function:</p>
<pre class="code literal-block"><span class="nb">FUNCTION</span><span class="p">(</span><span class="s">MY_FUNC</span> <span class="s">arg1</span><span class="p">)</span>
    <span class="nb">MESSAGE</span><span class="p">(</span><span class="s2">"arg1 = ${arg1}"</span><span class="p">)</span>
<span class="nb">ENDFUNCTION</span><span class="p">()</span>

<span class="nb">MY_FUNC</span><span class="p">(</span><span class="s">hello</span><span class="p">)</span>
<span class="nb">SET</span><span class="p">(</span><span class="s">var,</span> <span class="s">a</span><span class="p">)</span>
<span class="nb">MY_FUNC</span><span class="p">(</span><span class="s">var</span><span class="p">)</span> <span class="c"># arg1 is set to 'var'</span>
<span class="nb">MY_FUNC</span><span class="p">(</span><span class="o">${</span><span class="nv">var</span><span class="o">}</span><span class="p">)</span> <span class="c"># arg1 is set to 'a' - this is usually what you want</span>
</pre>


<p>produces</p>
<pre class="code literal-block"><span class="n">arg1</span> <span class="o">=</span> <span class="n">var</span>
<span class="n">arg1</span> <span class="o">=</span> <span class="n">a</span>
</pre>


<h3>Return values from functions</h3>
<p>There is no built-in notion of a return value from a function.   To get values out of a function, write to one of the arguments.</p>
<p>A function creates a new scope - changes to a variable will only affect the variable's value 
inside the function.  To affect the value in the parent, the <code>PARENT_SCOPE</code> modifier should be given to the <code>SET</code> command.  (More on variable scopes <a href="https://www.johnlamp.net/cmake-tutorial-5-functionally-improved-testing.html">here</a>)</p>
<p>Another issue is the variable name for the output value needs to be dereferenced before being set.
Otherwise a variable with the name used in the function will be set in the parent, which can work by accident
if the variables have the same name.</p>
<p>Example:</p>
<pre class="code literal-block"><span class="nb">FUNCTION</span><span class="p">(</span><span class="s">MY_FUNC_WITH_RET</span> <span class="s">ret</span><span class="p">)</span>
    <span class="c"># The following line works by accident if the name of variable in the parent</span>
    <span class="c"># is the same as in the function</span>
    <span class="nb">SET</span><span class="p">(</span><span class="s">ret</span> <span class="s2">"in function"</span> <span class="s">PARENT_SCOPE</span><span class="p">)</span>
    <span class="c"># This is the correct way to get the variable name passed to the function</span>
    <span class="nb">SET</span><span class="p">(</span><span class="o">${</span><span class="nv">ret</span><span class="o">}</span> <span class="s2">"in function"</span> <span class="s">PARENT_SCOPE</span><span class="p">)</span>
<span class="nb">ENDFUNCTION</span><span class="p">()</span>

<span class="nb">SET</span><span class="p">(</span><span class="s">ret</span> <span class="s2">"before function"</span><span class="p">)</span>
<span class="nb">MY_FUNC_WITH_RET</span><span class="p">(</span><span class="s">ret</span><span class="p">)</span>
<span class="nb">MESSAGE</span><span class="p">(</span><span class="s2">"output from function = ${ret}"</span><span class="p">)</span>
</pre>


<p>will produce <code>output from function = in function</code></p>
<h3>Data structures</h3>
<p>There is only the List type, with some functions for operations on lists.
The <a href="https://cmake.org/cmake/help/v3.3/manual/cmake-language.7.html#lists">documentation on lists</a> says that "Lists ... should not be used for complex data processing tasks", but doesn't say what to use instead.</p>
<p>For associative arrays or maps there are some options:</p>
<ul>
<li>Two parallel lists - one of keys and one of values.  Search the key list and use the index to look up value.  More awkward for passing to functions.</li>
<li>Single list with alternating keys and values.  Search the list for the key, and use index+1 to look up the value.  Only works if the range of possibilities for keys and values are distinct (e.g., keys are strings and values are always numbers).</li>
<li>The environment (<code>ENV{key}</code>) is a built-in associative array.  It could be overloaded to store other values, at the risk of polluting the environment.</li>
</ul>
<h3>Test timeout</h3>
<p>The default timeout per test is 1500 seconds (25 minutes).</p>
<p>To increase this, adjust the value of <code>DART_TESTING_TIMEOUT</code>.
It needs to be set as a cache variable, and it needs to be set before the <code>enable_testing()</code> or <code>include(CTest)</code> is specified.</p>
<pre class="code literal-block"><span class="nb">SET</span><span class="p">(</span> <span class="s">DART_TESTING_TIMEOUT</span> <span class="s">3600</span> <span class="s">CACHE</span> <span class="s">STRING</span> <span class="s2">"Maximum time for one test"</span><span class="p">)</span>
</pre>


<p>See also this <a href="http://stackoverflow.com/questions/3545598/using-cmake-with-ctest-and-cdash">Stack Overflow post</a></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/visualizing-md-data/" class="u-url">Visualizing MD Data</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">Mark Dewing</span></p>
            <p class="dateline"><a href="posts/visualizing-md-data/" rel="bookmark"><time class="published dt-published" datetime="2015-12-08T22:14:00-06:00" title="2015-12-08 22:14">2015-12-08 22:14</time></a></p>
                <p class="commentline">
        
    <a href="posts/visualizing-md-data/#disqus_thread" data-disqus-identifier="cache/posts/visualizing-md-data.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>Visualizing atomic positions in 3D is useful when working on a molecular dynamics code.</p>
<p>More generally, being able to visualize the structures and algorithms inside a code can help with understanding
and debugging.
With all the advances in graphics hardware, it should be possible to quickly create visualizations for various
aspects of the code.
But this seems harder than it should be.
In this particular case, normal plotting packages can sometimes plot atomic positions with 3D scatter plots.
But then further algorithm visualization is hard (animation, drawing boxes, etc).</p>
<p>The <a href="http://vispy.org/VisPy">VisPy</a> project looks promising.
It contains three API levels </p>
<ol>
<li>
<code>gloo</code>- the lowest level API around OpenGL</li>
<li>
<code>scene</code>- uses a scene graph description</li>
<li>
<code>plot</code> - standard plotting interface</li>
</ol>
<p>The 'scene' interface is the appropriate abstraction level for our purposes.
Note this API is marked experimental and may change in the future.</p>
<h3>Pretty pictures</h3>
<p>Static screenshots (click for larger version)</p>
<p><a href="2015/md_screenshot2.png"><img alt="Screenshot2" src="2015/md_screenshot2_sm.png"></a>
<a href="2015/md_screenshot3.png"><img alt="Screenshot3" src="2015/md_screenshot3_sm.png"></a></p>
<p>And an animation (click for larger version)</p>
<p><a href="2015/animation.gif"><img alt="Screenshot3" src="2015/animation_sm.gif"></a></p>
<h3>Code</h3>
<p>The modified <code>comd.py</code> is <a href="https://gist.github.com/markdewing/28223759c2dbe24e1147">here</a>.
It should be a drop-in replacement for that file in the <a href="https://github.com/markdewing/multitevo/tree/master/CoMD/python/nsquared"><code>nsquared</code></a> version of the code.  The bulk of the visualization additions start around line 154.</p>
<p>The perceived speed of the simulation can vary.  Pure Python, even at the smallest system size, is too slow.
Using the <a href="http://markdewing.github.io/blog/posts/first-performance-improvements/">Numba-accelerated</a> loop makes it much faster.
However, for the smallest system, this feels 'too fast'.
Increasing the system size will slow it down (`-x 6 -y 6 -z 6' seems to work well on my system).
There are much better ways of adjusting the speed, but this is the most expedient.</p>
<p>The <code>-N</code> option specifies the number of steps (defaults to 100).  Set it to a larger value to run the animation longer.</p>
<p>During the run, press <code>s</code> to take a static screenshot (stored in <code>screenshot.png</code>).  Press 'a' key to start/stop saving
an animated segment (stored in <code>animation.gif</code>).   These features require that the <code>imageio</code> package is installed.</p>
<p>The <code>multiprocessing</code> package is used to run the simulation and the visualization event loop in separate processes.
Positions are passed from the simulation to the visualization via a Queue.
A Timer on the visualization side checks the queue for new positions periodically.</p>
<p>This code snippet uses the Marker visual element to display the center of each point.
This size is the size of the element on the screen, not the size in the scene (that is, elements don't change size when zooming)
The current size was chosen to easily see the motion of the atoms, not to accurately represent the atom's size.
Displaying a Sphere at each point would be more accurate, but is much slower.</p>
<h3>Summary</h3>
<p>I'm very pleased with VisPy.  It enabled live, interactive animation of atoms from a molecular dynamics code with
 a small amount code.
I expect extending this to visualize more complex algorithms and data structures should be be straightforward.</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/more-with-numba/" class="u-url">More Performance With Numba</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">Mark Dewing</span></p>
            <p class="dateline"><a href="posts/more-with-numba/" rel="bookmark"><time class="published dt-published" datetime="2015-11-13T10:30:00-06:00" title="2015-11-13 10:30">2015-11-13 10:30</time></a></p>
                <p class="commentline">
        
    <a href="posts/more-with-numba/#disqus_thread" data-disqus-identifier="cache/posts/more-with-numba.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>Having acquired a shiny new <a href="http://markdewing.github.io/blog/posts/prototype-for-profiling-python/">profiler</a>, it's time to dig into the performance of the Numba version some more.</p>
<p>Picking up from the <a href="http://markdewing.github.io/blog/posts/improvements-in-comd-cell-method-performance/">previous optimizations</a>, I can't seem to reproduce the timing (47 μs/atom) in the that table.  Now I get 40 μs/atom.</p>
<h2>First step</h2>
<p>Run the profiler (<code>vmprofrun comd.py</code>) and display the results in KCacheGrind (<code>kcachegrind vmprof-20664.out</code>)</p>
<p>Sorting by self time, we see <code>getBoxFromCoord</code> at the top:</p>
<p><img alt="KCachegrind screenshot of functions sorted by self time" src="2015/profile1_by_self_sm.png"></p>
<p>Also a screen shot of the call graph - <code>getBoxFromCoord</code> gets called from two different places - <code>putAtomInBox</code> and <code>updateLinkCells</code>.</p>
<p><img alt="KCachegrind screenshot of call graph" src="2015/profile1_call_graph_sm.png"></p>
<p>To improve performance here, convert <code>getBoxFromCoord</code> to a free function and put all the attribute references into function arguments.</p>
<p>Before:</p>
<pre class="code literal-block">    <span class="k">def</span> <span class="nf">getBoxFromCoord</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">invBoxSize</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">iy</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">invBoxSize</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">iz</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">invBoxSize</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getBoxFromTuple</span><span class="p">(</span><span class="n">ix</span><span class="p">,</span> <span class="n">iy</span><span class="p">,</span> <span class="n">iz</span><span class="p">)</span>
</pre>


<p>After:</p>
<pre class="code literal-block"><span class="nd">@numba.njit</span>
<span class="k">def</span> <span class="nf">getBoxFromCoordInner</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">invBoxSize</span><span class="p">,</span> <span class="n">nLocalBoxes</span><span class="p">,</span> <span class="n">gs</span><span class="p">):</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">invBoxSize</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">iy</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">y</span><span class="o">*</span><span class="n">invBoxSize</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">iz</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">z</span><span class="o">*</span><span class="n">invBoxSize</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">getBoxFromTupleInner</span><span class="p">(</span><span class="n">ix</span><span class="p">,</span> <span class="n">iy</span><span class="p">,</span> <span class="n">iz</span><span class="p">,</span> <span class="n">nLocalBoxes</span><span class="p">,</span> <span class="n">gs</span><span class="p">)</span>
</pre>


<p>(Changing the parameter <code>r</code> to individual components was not strictly necessary.)</p>
<p>And the call sites change (for example) from</p>
<pre class="code literal-block">        <span class="n">iBox</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getBoxFromCoord</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">])</span>
</pre>


<p>to</p>
<pre class="code literal-block">        <span class="n">iBox</span> <span class="o">=</span> <span class="n">getBoxFromCoordInner</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">invBoxSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nLocalBoxes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">)</span>
</pre>


<p>This improves performance to 20 μs/atom.</p>
<p>Repeating the same transformation for putAtomInBox gives 18.4 μs/atom.</p>
<h2>Second step</h2>
<p>Run the profiler again.  By self time, <code>loadAtomsBuffer</code> is at the top.  Let's look at that in context.
Sort by inclusive time, and we see that the parts of the call tree starting at <code>redistributeAtoms</code> take a significant amount of time.</p>
<p><img alt="KCachegrind screenshot of functions sorted by inclusive time" src="2015/profile2_by_incl_sm.png"></p>
<p><img alt="KCachegrind screenshot of call graph" src="2015/profile2_call_graph_sm.png"></p>
<p>This part of the code:</p>
<ul>
<li>Applies periodic boundary conditions</li>
<li>Moves atoms to a new cell</li>
<li>Packs atom into a buffer at source</li>
<li>Unpacks buffer into atom data structure at destination</li>
</ul>
<p>This packing/unpacking anticipates the parallel version, which transmits the buffer across processors.</p>
<p>A previous attempt at using numpy records did not work well (and ran into a serious performance regression with numpy 1.10).
This time I went with two buffers - one for integers, and one for floating point numbers.  This works better, and the
performance is now 10.2 μs/atom.</p>
<h2>More steps</h2>
<p>Continuing the process of profiling, and converting routines to be Numba friendly eventually reached a performance of 2.9 μs/atom.
(Wow, this is only about 30% slower than C.)</p>
<p>Modified code is <a href="https://gist.github.com/markdewing/8bd6bd8dbef8613004fe">here</a></p>
<p>The updated performance table is</p>
<table>
<thead><tr>
<th>Language/compiler  </th>
<th>Version   </th>
<th align="right">Initial time</th>
<th align="right">  Final time</th>
<th></th>
</tr></thead>
<tbody>
<tr>
<td>Python</td>
<td>2.7.10</td>
<td align="right">1014</td>
<td align="right">1014</td>
<td></td>
</tr>
<tr>
<td>PyPy</td>
<td>4.0</td>
<td align="right">30</td>
<td align="right">30</td>
<td></td>
</tr>
<tr>
<td>Cython</td>
<td>0.23.3</td>
<td align="right">729</td>
<td align="right">13</td>
<td></td>
</tr>
<tr>
<td>Julia</td>
<td>0.4.0-rc3</td>
<td align="right">87</td>
<td align="right">6.1</td>
<td></td>
</tr>
<tr>
<td>Numba</td>
<td>0.22.1</td>
<td align="right">867</td>
<td align="right">2.9</td>
<td>    New result</td>
</tr>
<tr>
<td>C (clang)</td>
<td>3.7</td>
<td align="right">2.2</td>
<td align="right">2.2</td>
<td></td>
</tr>
</tbody>
</table>
<p><br></p>
<div style="font-size:80%">
Times are all in μs/atom. System size is 4000 atoms.
Hardware is Xeon E5-2630 v3 @ 2.4 Ghz, OS is Ubuntu 12.04.
<br>
The 'Initial Time' column results from the minimal amount of code changes to get the compiler working.
<br>
The 'Final Time' is the time after tuning.
</div>
<br><p>Do note that the Cython, Julia, and Numba results reflect the amount of effort put into optimization.
Cython and Julia still need to be improved with the assistance of a profiler (or in Julia's case, a better viewer
for existing profile data).</p>
<h2>Summary</h2>
<p>Using a profiler to guide our optimization efforts has been very helpful.</p>
<p>The Numba results are really promising, but, in addition to creating ugly code, it required an amount of work
that I would not want to perform on a regular basis.    These transformations are fairly regular, so it should
be possible to incorporate them into Numba.  Alternately, if doing so in a safe manner inside the compiler is difficult,
some sort of automated AST transformation of the source should be possible.</p>
<p>As the optimization process proceeds on this code, increasing amount of time is being spent in the core routine, computeForce, (as it should), and we will need to move beyond a function-level profiler to look for optimization opportunities.</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/updated-performance-with-pypy-40/" class="u-url">Performance Updates with PyPy 4.0</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">Mark Dewing</span></p>
            <p class="dateline"><a href="posts/updated-performance-with-pypy-40/" rel="bookmark"><time class="published dt-published" datetime="2015-11-04T15:39:00-06:00" title="2015-11-04 15:39">2015-11-04 15:39</time></a></p>
                <p class="commentline">
        
    <a href="posts/updated-performance-with-pypy-40/#disqus_thread" data-disqus-identifier="cache/posts/updated-performance-with-pypy-40.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>The PyPy team recently released version <a href="http://morepypy.blogspot.com/2015/10/pypy-400-released-jit-with-simd.html">4.0</a>
(The jump in version number is to reduce confusion with the Python version supported.)
One of the features is improved performance.</p>
<p>But first, an issue with reporting accurate timings with this version of CoMD should be addressed. The initial iteration contains overhead from tracing and JIT compilation (Cython and Numba have the same issue).
For this example we are concerned with the steady-state timing, so the time for the first iteration should be excluded.
I've added a '--skip' parameter to the CoMD code (default: 1) that skips the first <code>printRate</code> steps (default: 10) in computing the overall average update rate at the end.</p>
<p>Now the table with the most recent performance numbers  (from <a href="http://markdewing.github.io/blog/posts/improvements-in-comd-cell-method-performance/">this post</a> ), updated with PyPy 4.0 results:</p>
<table>
<thead><tr>
<th>Language/compiler  </th>
<th>Version   </th>
<th align="right">Time</th>
<th align="right"></th>
</tr></thead>
<tbody>
<tr>
<td>Python</td>
<td>2.7.10</td>
<td align="right">1014</td>
<td align="right"></td>
</tr>
<tr>
<td>PyPy</td>
<td>2.6.1</td>
<td align="right">96</td>
<td align="right"></td>
</tr>
<tr>
<td>Numba</td>
<td>0.21.0</td>
<td align="right">47</td>
<td align="right"></td>
</tr>
<tr>
<td>PyPy</td>
<td>4.0</td>
<td align="right">30</td>
<td align="right">    New result</td>
</tr>
<tr>
<td>Cython</td>
<td>0.23.3</td>
<td align="right">13</td>
<td align="right"></td>
</tr>
<tr>
<td>Julia</td>
<td>0.4.0-rc3</td>
<td align="right">6.1</td>
<td align="right"></td>
</tr>
<tr>
<td>C</td>
<td>4.8.2</td>
<td align="right">2.3</td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p><br>
Times are all in μs/atom. System size is 4000 atoms.
Hardware is Xeon E5-2630 v3 @ 2.4 Ghz, OS is Ubuntu 12.04.
<br></p>
<p>The new release of PyPy also includes some SIMD vectorization support (<code>--jit vec=1</code> or <code>--jit vec_all=1</code>).  Neither of these provided any improvement in performance on this code.   Not too surprising given the vectorization support is new, and the code contains
conditionals in the inner loop.</p>
<p>PyPy 4.0 gives a very good 34x performance improvement over bare Python, and 3x improvement over the previous release (2.6.1).
PyPy is attractive here in that no modifications made to the source.  (Both Cython and Numba required source changes)</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/prototype-for-profiling-python/" class="u-url">Prototype for Profiling Python</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">Mark Dewing</span></p>
            <p class="dateline"><a href="posts/prototype-for-profiling-python/" rel="bookmark"><time class="published dt-published" datetime="2015-10-12T12:20:00-05:00" title="2015-10-12 12:20">2015-10-12 12:20</time></a></p>
                <p class="commentline">
        
    <a href="posts/prototype-for-profiling-python/#disqus_thread" data-disqus-identifier="cache/posts/prototype-for-profiling-python.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p><a href="posts/towards-profiling-accelerated-python/">Last post</a> covered some technical background using vmprof to profile Python with compiled or JIT'ed extensions.
Now I've created a prototype that can convert the output to callgrind format so it can be viewed with <a href="http://kcachegrind.sourceforge.net/html/Home.html">KCachegrind</a>.</p>
<p>To install the prototype using the Anaconda distribution:</p>
<ol>
<li>Create a new environment (if you do not use a new environment, these packages may conflict with an existing Numba install): <code>conda create -n profiling python numpy</code>
</li>
<li>Switch to the new environment: <code>source activate profiling</code>
</li>
<li>Install prototype versions of Numba and llvmlite: <code>conda install -c https://conda.anaconda.org/mdewing numba-profiling</code>
</li>
<li>Install prototype version of vmprof: <code>conda install -c https://conda.anaconda.org/mdewing vmprof-numba</code>
</li>
<li>Make sure libunwind is installed.  (On Ubuntu <code>apt-get install libunwind8-dev</code>.)
(On Ubuntu, it must be the -dev version.  If not installed, the error message when trying to run vmprof is <code>ImportError: libunwind.so.8: cannot open shared object file: No such file or directory</code>)</li>
<li>Install KCachegrind (On Ubuntu, <code>apt-get install kcachegrind</code>)</li>
</ol>
<p>There is a wrapper (<code>vmprofrun</code>) that automates the running and processing steps.
To use it, run <code>vmprofrun &lt;target python script&gt; [arguments to the python script]</code>. 
(No need to specify <code>python</code> - that gets added to the command line automatically.)
By default it will output <code>vmprof-&lt;pid&gt;.out</code>, which can be viewed in KCachegrind.</p>
<p>Underneath, the <code>vmprofrun</code> tool saves the vmprof output during the run to <code>out.vmprof</code>. After the run, it automatically copies the <code>/tmp/perf-&lt;pid&gt;.map</code> file to the current directory (if running under Numba).
It moves <code>out.vmprof</code> to <code>out-&lt;pid&gt;.vmprof</code>.
Finally it runs <code>vmproftocallgrind</code> using these files as input.</p>
<h4>Limitations</h4>
<ol>
<li>Only works on 64-bit Linux</li>
<li>Function-level profiles only - no line information (for either python or native code)</li>
<li>Sometimes the profiling hangs during the run - kill the process and try again.</li>
<li>Works with Python 2.7 or 3.4</li>
<li>Not well validated or tested yet</li>
<li>It does not work well yet with the existing vmprof web visualization and CLI tools.</li>
</ol>
<p>Other notes:</p>
<ul>
<li>The stack dump tool will process the stacks to remove the Python interpreter frames.</li>
<li>By default the Numba <code>Dispatcher_call</code> level is removed.  Otherwise the call graph in KCachegrind gets tangled by all the call paths running through this function.</li>
<li>It should work with C extensions and Cython as well.</li>
</ul>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/towards-profiling-accelerated-python/" class="u-url">Towards Profiling Accelerated Python</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">Mark Dewing</span></p>
            <p class="dateline"><a href="posts/towards-profiling-accelerated-python/" rel="bookmark"><time class="published dt-published" datetime="2015-10-07T20:58:00-05:00" title="2015-10-07 20:58">2015-10-07 20:58</time></a></p>
                <p class="commentline">
        
    <a href="posts/towards-profiling-accelerated-python/#disqus_thread" data-disqus-identifier="cache/posts/towards-profiling-accelerated-python.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>One of the conclusions from last post is a need for better profiling tools to show where time is spent in the code.
Profiling Python + JIT'ed code requires dealing with a couple of issues.</p>
<p>The first issue is collecting stack information at different language levels.
A native profiler collects a stack for the JIT'ed (or compiled extension) code, but eventually the stack enters the implementation of the Python interpreter loop.
Unless we are trying to optimized the interpreter loop, this is not useful.
We would rather know what Python code is being executed.
Python profilers can collect the stack at the Python level, but can't collect native code stacks.</p>
<p>The PyPy developers created a solution in <a href="https://vmprof.readthedocs.org/en/latest/">vmprof</a>.
It walks the stack like a native profiler, but also hooks the Python interpreter
so that it can collect the Python code's file, function, and line number.
This solution is general to any type of compiled extension (C extensions, Cython, Numba, etc.)
Read the section in the vmprof docs on <a href="https://vmprof.readthedocs.org/en/latest/#why-a-new-profiler">Why a new profiler?</a> for more information.</p>
<p>The second issue is particular to JIT'ed code - resolving symbol information after the run.
For low overhead, native profilers collect a minimum of information at runtime (usually the Instruction Pointer (IP) address at each stack level).
These IP addresses need to resolved to symbol information after collection.
Normally this information is kept in debug sections that are generated at compile time.
However, with JIT compilation, the functions and their address mappings are generated at runtime.</p>
<p>LLVM includes an interface to get symbol information at runtime.
The simplest way to keep it for use after the run is to follow the Linux perf standard (documented <a href="https://github.com/torvalds/linux/blob/master/tools/perf/Documentation/jit-interface.txt">here</a>), which stores the address, size, and function name in a file <code>/tmp/perf-&lt;pid&gt;.map</code>.</p>
<p>To enable Numba with vmprof, I've created a version of llvmlite that is amenable to stack collection, at the <em>perf</em> branch <a href="https://github.com/markdewing/llvmlite/tree/perf">here</a>.</p>
<p>This does two things:</p>
<ol>
<li>Keep the frame pointer in JIT'ed code, so a backtrace can be taken.<sup id="fnref:1"><a class="footnote-ref" href="posts/towards-profiling-accelerated-python/#fn:1" rel="footnote">1</a></sup>
</li>
<li>Output a perf-compatible JIT map file (not on by default - need to call <code>enable_jit_events</code> to turn it on)</li>
</ol>
<p>To use this, modify Numba to enable JIT events and frame pointers:</p>
<ol>
<li>In  <code>targets\codegen.py</code>, at the end of the <code>_init</code> method of <code>BaseCPUCodegen</code>, add <code>self._engine.enable_jit_events()</code>
</li>
<li>And for good measure, turn on frame pointers for Numba code as well (set <code>CFLAGS=-fno-omit-frame-pointer</code> before building it)</li>
</ol>
<p>The next piece is a modified version of vmprof ( in branch <a href="https://github.com/markdewing/vmprof-python/tree/numba"><em>numba</em></a> ).
So far all it does is read the perf compatible output and dump raw stacks.
Filtering and aggregating Numba stacks remains to be done (meaning neither the CLI nor the GUI display work yet).</p>
<p>How to use what works, so far:</p>
<ol>
<li>Run vmprof, using perf-enabled Numba above:  <code>python -m vmprof -o vmprof.out &lt;target python&gt;</code>
</li>
<li>Copy map file <code>/tmp/perf-&lt;pid&gt;.map</code> to some directory.   I usually copy <code>vmprof.out</code> to something like <code>vmprof-&lt;pid&gt;.out</code> to remember which files correlate.</li>
<li>View raw stacks with <code>vmprofdump vmprof-&lt;pid&gt;.out --perf perf-&lt;pid&gt;.map</code>.  </li>
</ol>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>With x86_64, it is possible to use DWARF debug information to walk the stack.  I couldn't figure out how to output the appropriate debug information.  LLVM 3.6 has a promising target option named <code>JITEmitDebugInfo</code>.  However, <code>JITEmitDebugInfo</code> is a lie!  It's not hooked up to anything, and has been removed in LLVM 3.7. <a class="footnote-backref" href="posts/towards-profiling-accelerated-python/#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
</ol>
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/improvements-in-comd-cell-method-performance/" class="u-url">Improvements in CoMD Cell Method Performance</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">Mark Dewing</span></p>
            <p class="dateline"><a href="posts/improvements-in-comd-cell-method-performance/" rel="bookmark"><time class="published dt-published" datetime="2015-10-02T13:56:00-05:00" title="2015-10-02 13:56">2015-10-02 13:56</time></a></p>
                <p class="commentline">
        
    <a href="posts/improvements-in-comd-cell-method-performance/#disqus_thread" data-disqus-identifier="cache/posts/improvements-in-cell-method-performance.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>A <a href="http://markdewing.github.io/blog/posts/first-performance-improvements/">previous post</a> showed some performance improvements with the <em>nsquared</em> version of the code.
This post will tackle the <em>cell</em> version of the code.
In the <em>nsquared</em> version the time-consuming inner loop had no function calls.
The <em>cell</em> version does call other functions, which may complicate optimization.</p>
<table>
<thead><tr>
<th>Language/compiler  </th>
<th>Version   </th>
<th align="right">Initial time</th>
<th align="right">  Final time</th>
</tr></thead>
<tbody>
<tr>
<td>C</td>
<td>4.8.2</td>
<td align="right">2.3</td>
<td align="right">2.3</td>
</tr>
<tr>
<td>Python</td>
<td>2.7.10</td>
<td align="right">1014</td>
<td align="right">1014</td>
</tr>
<tr>
<td>PyPy</td>
<td>2.6.1</td>
<td align="right">96</td>
<td align="right">96</td>
</tr>
<tr>
<td>Julia</td>
<td>0.4.0-rc3</td>
<td align="right">87</td>
<td align="right">6.1</td>
</tr>
<tr>
<td>Cython</td>
<td>0.23.3</td>
<td align="right">729</td>
<td align="right">13</td>
</tr>
<tr>
<td>Numba</td>
<td>0.21.0</td>
<td align="right">867</td>
<td align="right">47</td>
</tr>
</tbody>
</table>
<p><br>
Times are all in μs/atom. System size is 4000 atoms.
Hardware is Xeon E5-2630 v3 @ 2.4 Ghz, OS is Ubuntu 12.04.
<br>
The 'Initial Time' column results from the minimal amount of code changes to get the compiler working.
<br>
The 'Final Time' is the time after the tuning in this post.</p>
<h3>Julia</h3>
<p>The <em>cell</em> version contains the same issue with array operations as the <em>nsquared</em> version - the computation of <code>dd</code> allocates a temporary to hold the results every time through the loop.</p>
<p>The <a href="https://github.com/lindahua/Devectorize.jl">Devectorize</a> package can automatically convert array notation
to a loop.  If we add the <code>@devec</code> annotation, the time drops to 43 μs/atom.
Unfortunately, the allocation to hold the result must still be performed, and it remains inside the inner particle loop.
If we manually create the loop and hoist the allocation out of the loop, time is 27 μs/atom.</p>
<p>The code uses <code>dot</code> to compute the vector norm.  This calls a routine (<code>julia_dot</code>) to perform the
dot product.
For long vectors calling an optimized linear algebra routine is beneficial, but for a vector of length 3 this adds overhead.
Replacing <code>dot</code> with the equivalent loop reduces the time to 23 μs/atom.</p>
<p>Looking through the memory allocation output (<code>--track-allocation=user</code>) shows some vector operations
when the force array is zeroed and accumulated.
Also in <code>putAtomInBox</code> in <code>linkcell.jl</code>.
 These spots are also visible in the profile output, but the profile output is less convenient because it is not shown with source.
The <code>@devec</code> macro does work here, and the performance is now 7.7 μs/atom.   Explicit loops
give a slightly better time of 7.3 μs/atom.</p>
<p>Profiling shows even more opportunities for devectorization in <code>advanceVelocity</code> and <code>advancePosition</code> in <code>simflat.jl</code>  Time is now 6.4 μs/atom.</p>
<p>The Julia executable has a <code>-O</code> switch for more time-intensive optimizations (it adds more LLVM optimization passes).   This improves the time to 6.2 μs/atom.</p>
<p>The <code>@fastmath</code> macro improves the time a little more, to 6.1 μs/atom.
The <code>@inbounds</code> macro to skip the bounds checks did not seem to improve the time.</p>
<p>The final Julia time is now within a factor of 3 of the C time.  The code is <a href="https://gist.github.com/markdewing/54709a0fd6a17348a7cb">here</a>.  It's not clear where the remaining time overhead comes from. </p>
<h3>PyPy</h3>
<p>The PyPy approach to JIT compilation is very general, but that also makes it difficult to target what code
changes might improve performance.
The <a href="https://bitbucket.org/pypy/jitviewer">Jitviewer</a> tool is nice, but not helpful at a cursory glance.
The <a href="https://vmprof.readthedocs.org/en/latest/">vmprof</a> profiler solves an important problem by collecting the native code stack plus the python stack. 
In this particular case, it reports at the function level, and the bulk of the time was spent in <code>computeForce</code>.
I hope to write more about vmprof in a future post, as it could help with integrated profiling of Python + native code (either compiled or JIT-ed).</p>
<h3>Cython</h3>
<p>The simplest step is to add an initialization line and move some <code>.py</code> files to <code>.pyx</code> files.  This gives 729 μs/atom.
Adding types to the computeForce function and assigning a few attribute lookups to local variables so the types can be assigned (playing a game of 'remove the yellow' in the Cython annotation output) gives 30 μs/atom.</p>
<p>Adding types and removing bounds checks more routines  (in  <code>halo.py</code>, <code>linkcell.py</code>, <code>simflat.py</code>) gives 13 μs/atom.</p>
<p>Code is <a href="https://gist.github.com/markdewing/3688c6eebc0a88081e07">here</a>.
Further progress needs deeper investigation with profiling tools.</p>
<h3>Numba</h3>
<p>Starting with adding <code>@numba.jit</code> decorators to <code>computeForce</code>, and the functions it calls gives the
initial time of 867 μs/atom.
Extracting all the attribute lookups (including the function call to <code>getNeighborBoxes</code>) gives 722 μs/atom.</p>
<p>We should ensure the call to <code>getNeighborBoxes</code> is properly JIT-ed.  Unfortunately, this requires more involved
code restructuring.  Functions need to be split into a wrapper that performs any needed attribute lookups, and
an inner function that gets JIT-ed.  Loop lifting automatically performs this transformation on functions
  with loops.  On functions without loops, however, it needs to be done manually.
Once this is done, the time improves dramatically to 47 μs/atom.</p>
<p>Hopefully the upcoming "JIT Classes" feature will make this easier, and require less code restructuring. </p>
<p>Code is <a href="https://gist.github.com/markdewing/89cce577f5b8625cc776">here</a></p>
<h3>Summary</h3>
<p>Julia is leading in terms of getting the best performance on this example.  Many of these projects are rapidly improving, so this is just a snapshot at their current state.</p>
<p>All these projects need better profiling tools to show the user where code is slow and to give feedback on why the code is slow.
The Cython annotated output is probably the the best - it highlights which lines need attention. 
However it is not integrated with profiler output, so in a project of any size, it's not clear where a user should spend time adding types.  </p>
<p>Julia has some useful collection and feedback tools, but they would be much more helpful if combined.  The memory allocation output is bytes allocated.
It's useful for finding allocations where none were expected, or for allocations in known hot loops, but it's less clear which other allocations are impacting performance.
Ideally this could be integrated with profiler output and weighted by time spent to show which allocations are actually affecting execution time.</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/two-meanings-of-vectorization/" class="u-url">Two Meanings of Vectorization</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">Mark Dewing</span></p>
            <p class="dateline"><a href="posts/two-meanings-of-vectorization/" rel="bookmark"><time class="published dt-published" datetime="2015-10-01T22:23:00-05:00" title="2015-10-01 22:23">2015-10-01 22:23</time></a></p>
                <p class="commentline">
        
    <a href="posts/two-meanings-of-vectorization/#disqus_thread" data-disqus-identifier="cache/posts/two-meanings-of-vectorization.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>The term 'vectorize' as used by programmers has at least two separate uses.
Both uses can have implications for performance, which sometimes leads to confusion.</p>
<p>One meaning refers to a language syntax to express operations on multiple values - typically an entire array, or a slice of a array.
This can be a very convenient notation for expressing algorithms.</p>
<p>Here is a simple example (in Julia) using loop-oriented (devectorized) notation</p>
<pre class="code literal-block"><span class="n">a</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="kt">Float64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="kt">Float64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="c"># allocate space for result</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="kt">Float64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="p">:</span><span class="mi">10</span>
    <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="k">end</span>
</pre>


<p>Now compare with using vectorized (array) notation</p>
<pre class="code literal-block"><span class="n">a</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="kt">Float64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="kt">Float64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c"># space for result automatically allocated</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</pre>


<p>The vectorized notation is more compact.
Julia and Python/Numpy programmers usually mean this when referring to 'vectorization'.
See more in the Wikipedia entry on <a href="https://en.wikipedia.org/wiki/Array_programming">Array programming</a></p>
<p>John Myles White wrote a post discussing the performance implications of <a href="http://www.johnmyleswhite.com/notebook/2013/12/22/the-relationship-between-vectorized-and-devectorized-code/">vectorized and devectorized code</a> in Julia and R.
Note that Python/Numpy operates similar to R as described in the post - good performance usually requires appropriately vectorized code, because that skips the interpreter and calls higher performing C routines underneath.</p>
<p>The other meaning of 'vectorization' refers to generating assembly code to make effective use of fine-grained parallelism in hardware SIMD units.
This is what Fortran or C/C++ programmers (and their compilers) mean by 'vectorization'.
In Julia, the <code>@simd</code> macro gives hints to the compiler that a given loop can be vectorized.</p>
<p>See more in the Wikipedia entry on <a href="https://en.wikipedia.org/wiki/Automatic_vectorization">Automatic vectorization</a></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/why-types-help-performance/" class="u-url">Why Types Help Performance</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">Mark Dewing</span></p>
            <p class="dateline"><a href="posts/why-types-help-performance/" rel="bookmark"><time class="published dt-published" datetime="2015-09-24T10:48:00-05:00" title="2015-09-24 10:48">2015-09-24 10:48</time></a></p>
                <p class="commentline">
        
    <a href="posts/why-types-help-performance/#disqus_thread" data-disqus-identifier="cache/posts/why-types-help-performance.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>In <a href="http://markdewing.github.io/blog/posts/first-performance-improvements">previous</a> <a href="http://markdewing.github.io/blog/posts/comparing-languages-with-miniapps/">posts</a>, we've seen that adding type information can help the performance of the code generated by dynamic language compilers.
The documentation for Cython annotations talks of 'Python interaction', and Numba has 'object' mode and 'nopython' modes.
In post I will look more at what these mean, and how they affect performance.</p>
<p>To start, consider how values are represented in a computer, such as a simple integer ('int' type in C).
The bare value takes 4 bytes in memory, and no additional information about it is stored,
such as its type or how much space it occupies.
This information is all implicit at run time <sup id="fnref:1"><a class="footnote-ref" href="posts/why-types-help-performance/#fn:1" rel="footnote">1</a></sup>.
That it takes 4-bytes and is interpreted as an integer is determined at compile time.</p>
<p>In dynamic languages, this extra information can be queried at run-time.
For example:</p>
<pre class="code literal-block"><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="mi">10</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">type</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="o">&lt;</span><span class="nb">type</span> <span class="s">'int'</span><span class="o">&gt;</span>
</pre>


<p>The value stored in <code>a</code> has type <code>int</code>, Python's integer type.
This extra information must be stored somewhere, and languages often solve this by wrapping
the bare value in an object.
This is usually called 'boxing'.
The value plus type information (and any other information) is called a 'boxed type'.
The bare value is called an 'unboxed type' or a 'primitive value'.</p>
<p>In Java, different types can be explicitly created (<code>int</code> vs. <code>Integer</code>), and the programmer
needs to know the differences and tradeoffs.
(See this <a href="http://stackoverflow.com/questions/13055/what-is-boxing-and-unboxing-and-what-are-the-trade-offs">Stack Overflow question</a> for more about boxed and primitive types.)</p>
<p>Python only has boxed values ('everything is an object'). From the interpreter, this means we can
always determine the type of a value.
If we look a layer down, as would be needed to integrate with C, these values are accessed through the Python API.
The base type of any object is PyObject.  For our simple example, integers are stored as PyIntObject.</p>
<p>For example, consider the following Python code.</p>
<pre class="code literal-block">  <span class="k">def</span> <span class="nf">add</span><span class="p">():</span>
    <span class="n">a</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</pre>


<p>One way to see what calls the interpreter would make is to compile with Cython.
The following C is the result (simplified - reference counting pieces to the Python API are omitted.)</p>
<pre class="code literal-block"><span class="n">PyObject</span> <span class="o">*</span><span class="nf">add</span><span class="p">()</span>
<span class="p">{</span>
  <span class="n">PyObject</span> <span class="o">*</span><span class="n">pyx_int_1</span> <span class="o">=</span> <span class="n">PyInt_FromLong</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
  <span class="n">PyObject</span> <span class="o">*</span><span class="n">pyx_int_2</span> <span class="o">=</span> <span class="n">PyInt_FromLong</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
  <span class="n">PyObject</span> <span class="o">*</span><span class="n">pyx_tmp</span> <span class="o">=</span> <span class="n">PyNumber_Add</span><span class="p">(</span><span class="n">pyx_int_1</span><span class="p">,</span> <span class="n">pyx_int_2</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">pyx_tmp</span><span class="p">;</span>
<span class="p">}</span>
</pre>


<p>Without type information, Cython basically unrolls the interpreter loop, and makes
a sequence of Python API calls.
The HTML annotation output highlights lines with Python interaction, and can be expanded to show
the generated code.   This gives feedback on where and what types need to be added to avoid
the Python API calls.</p>
<p>Add some Cython annotations and the example becomes</p>
<pre class="code literal-block"> <span class="n">cdef</span> <span class="nb">int</span> <span class="n">add</span><span class="p">():</span>
    <span class="n">cdef</span> <span class="nb">int</span> <span class="n">a</span>
    <span class="n">cdef</span> <span class="nb">int</span> <span class="n">b</span>
    <span class="n">a</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</pre>


<p>Now the following code is generated</p>
<pre class="code literal-block"> <span class="kt">int</span> <span class="nf">add</span><span class="p">()</span>
 <span class="p">{</span>
  <span class="kt">int</span> <span class="n">a</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
  <span class="k">return</span> <span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="p">;</span>
 <span class="p">}</span>
</pre>


<p>The add of the two integers is done directly (and runs much faster), rather than going through the Python API call.</p>
<h3>Numba</h3>
<p>If its type information is insufficient, Numba will call the Python API for every operation.
Since all the operations occur on Python objects, this is called 'object' mode (slow).
With sufficient type information, code can be generated with no calls to the Python API, and hence
the name 'nopython' mode (fast).</p>
<h3>Julia</h3>
<p>Julia has boxed object types, but is designed to try use the unboxed types as much as possible.
The most generic type is called 'Any', and it is possible to produce Julia code that runs this mode.<br>
See the section on <a href="http://julia.readthedocs.org/en/latest/manual/embedding/">Embedding Julia</a> in the
documentation for more about Julia objects.</p>
<p>Julia's type inference only happens inside functions.
This is why composite types (structures) need type annotations for good performance.</p>
<p>This example demonstrates the issue</p>
<pre class="code literal-block"><span class="k">type</span><span class="nc"> Example</span>
    <span class="n">val</span>
<span class="k">end</span>

<span class="k">function</span><span class="nf"> add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">val</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">val</span>
<span class="k">end</span>

<span class="n">v1</span> <span class="o">=</span> <span class="n">Example</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">v2</span> <span class="o">=</span> <span class="n">Example</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="p">@</span><span class="n">code_llvm</span> <span class="n">add</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span>  <span class="c"># The @code_llvm macro prints the LLVM IR.  </span>
<span class="n">t</span> <span class="o">=</span> <span class="n">add</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">"res = "</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
</pre>


<p>Since the type of the 'val' element is not known, the code operates on a generic object type <code>jl_value_t</code> and eventually calls <code>jl_apply_generic</code>, which looks up the right method and dispatches to it at execution time.
(The LLVM IR is not shown here - run the example to see it.)  Doing all this at execution time is slow.</p>
<p>Now add the type annotation</p>
<pre class="code literal-block"><span class="k">type</span><span class="nc"> Example</span>
    <span class="n">val</span><span class="p">::</span><span class="kt">Int</span>
<span class="k">end</span>
</pre>


<p>The resulting LLVM IR (also not shown here) is much shorter in that it adds two integers directly and
returns the result as an integer.
With type information, the lookup and dispatch decisions can be made at compile time.</p>
<p>Note that Julia uses a Just In Time (JIT) compiler, which means compilation occurs at run time.
The run time can be split into various phases, which include compilation and execution of
the resulting code.</p>
<h3>Summary</h3>
<p>Hopefully this post sheds some light on how type information can affect the performance of dynamic languages.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>The type can be accessible at run time via debug information.  See this Strange Loop 2014 talk: <a href="https://www.youtube.com/watch?v=LwicN2u6Dro">Liberating the lurking Smalltalk in Unix</a> <a class="footnote-backref" href="posts/why-types-help-performance/#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
</ol>
</div>
</div>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="next">
                <a href="index-1.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="journeytothecenterofthecomputer";(function(){var a=document.createElement("script");a.async=true;a.src="//"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2016         <a href="mailto:markdewing%20(at)%20gmail%20(dot)%20com">Mark Dewing</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>
