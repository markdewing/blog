<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Journey to the Center of the Computer</title><link>https://markdewing.github.io/blog/</link><description>Investigations into hardware and software details</description><atom:link href="https://markdewing.github.io/blog/rss.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Thu, 08 Oct 2015 02:08:09 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Towards Profiling Accelerated Python</title><link>https://markdewing.github.io/blog/posts/towards-profiling-accelerated-python/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;One of the conclusions from last post is a need for better profiling tools to show where time is spent in the code.
Profiling Python + JIT'ed code requires dealing with a couple of issues.&lt;/p&gt;
&lt;p&gt;The first issue is collecting stack information at different language levels.
A native profiler collects a stack for the JIT'ed (or compiled extension) code, but eventually the stack enters the implementation of the Python interpreter loop.
Unless we are trying to optimized the interpreter loop, this is not useful.
We would rather know what Python code is being executed.
Python profilers can collect the stack at the Python level, but can't collect native code stacks.&lt;/p&gt;
&lt;p&gt;The PyPy developers created a solution in &lt;a href="https://vmprof.readthedocs.org/en/latest/"&gt;vmprof&lt;/a&gt;.
It walks the stack like a native profiler, but also hooks the Python interpreter
so that it can collect the Python code's file, function, and line number.
This solution is general to any type of compiled extension (C extensions, Cython, Numba, etc.)
Read the section in the vmprof docs on &lt;a href="https://vmprof.readthedocs.org/en/latest/#why-a-new-profiler"&gt;Why a new profiler?&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;The second issue is particular to JIT'ed code - resolving symbol information after the run.
For low overhead, native profilers collect a minimum of information at runtime (usually the Instruction Pointer (IP) address at each stack level).
These IP addresses need to resolved to symbol information after collection.
Normally this information is kept in debug sections that are generated at compile time.
However, with JIT compilation, the functions and their address mappings are generated at runtime.&lt;/p&gt;
&lt;p&gt;LLVM includes an interface to get symbol information at runtime.
The simplest way to keep it for use after the run is to follow the Linux perf standard (documented &lt;a href="https://github.com/torvalds/linux/blob/master/tools/perf/Documentation/jit-interface.txt"&gt;here&lt;/a&gt;), which stores the address, size, and function name in a file &lt;code&gt;/tmp/perf-&amp;lt;pid&amp;gt;.map&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To enable Numba with vmprof, I've created a version of llvmlite that is amenable to stack collection, at the &lt;em&gt;perf&lt;/em&gt; branch &lt;a href="https://github.com/markdewing/llvmlite/tree/perf"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This does two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Keep the frame pointer in JIT'ed code, so a backtrace can be taken.&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://markdewing.github.io/blog/posts/towards-profiling-accelerated-python/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;Output a perf-compatible JIT map file (not on by default - need to call &lt;code&gt;enable_jit_events&lt;/code&gt; to turn it on)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To use this, modify Numba to enable JIT events and frame pointers:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In  &lt;code&gt;targets\codegen.py&lt;/code&gt;, at the end of the &lt;code&gt;_init&lt;/code&gt; method of &lt;code&gt;BaseCPUCodegen&lt;/code&gt;, add &lt;code&gt;self._engine.enable_jit_events()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;And for good measure, turn on frame pointers for Numba code as well (set &lt;code&gt;CFLAGS=-fno-omit-frame-pointer&lt;/code&gt; before building it)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The next piece is a modified version of vmprof ( in branch &lt;a href="https://github.com/markdewing/vmprof-python/tree/numba"&gt;&lt;em&gt;numba&lt;/em&gt;&lt;/a&gt; ).
So far all it does is read the perf compatible output and dump raw stacks.
Filtering and aggregating Numba stacks remains to be done (meaning neither the CLI nor the GUI display work yet).&lt;/p&gt;
&lt;p&gt;How to use what works, so far:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run vmprof, using perf-enabled Numba above:  &lt;code&gt;python -m vmprof -o vmprof.out &amp;lt;target python&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Copy map file &lt;code&gt;/tmp/perf-&amp;lt;pid&amp;gt;.map&lt;/code&gt; to some directory.   I usually copy &lt;code&gt;vmprof.out&lt;/code&gt; to something like &lt;code&gt;vmprof-&amp;lt;pid&amp;gt;.out&lt;/code&gt; to remember which files correlate.&lt;/li&gt;
&lt;li&gt;View raw stacks with &lt;code&gt;vmprofdump vmprof-&amp;lt;pid&amp;gt;.out --perf perf-&amp;lt;pid&amp;gt;.map&lt;/code&gt;.  &lt;/li&gt;
&lt;/ol&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;With x86_64, it is possible to use DWARF debug information to walk the stack.  I couldn't figure out how to output the appropriate debug information.  LLVM 3.6 has a promising target option named &lt;code&gt;JITEmitDebugInfo&lt;/code&gt;.  However, &lt;code&gt;JITEmitDebugInfo&lt;/code&gt; is a lie!  It's not hooked up to anything, and has been removed in LLVM 3.7. &lt;a class="footnote-backref" href="https://markdewing.github.io/blog/posts/towards-profiling-accelerated-python/#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>Numba</category><category>PyPy</category><category>python</category><category>vmprof</category><guid>https://markdewing.github.io/blog/posts/towards-profiling-accelerated-python/</guid><pubDate>Thu, 08 Oct 2015 01:58:00 GMT</pubDate></item><item><title>Improvements in CoMD Cell Method Performance</title><link>https://markdewing.github.io/blog/posts/improvements-in-comd-cell-method-performance/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;A &lt;a href="http://markdewing.github.io/blog/posts/first-performance-improvements/"&gt;previous post&lt;/a&gt; showed some performance improvements with the &lt;em&gt;nsquared&lt;/em&gt; version of the code.
This post will tackle the &lt;em&gt;cell&lt;/em&gt; version of the code.
In the &lt;em&gt;nsquared&lt;/em&gt; version the time-consuming inner loop had no function calls.
The &lt;em&gt;cell&lt;/em&gt; version does call other functions, which may complicate optimization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language/compiler  &lt;/th&gt;
&lt;th&gt;Version   &lt;/th&gt;
&lt;th align="right"&gt;Initial time&lt;/th&gt;
&lt;th align="right"&gt;  Final time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;C&lt;/td&gt;
&lt;td&gt;4.8.2&lt;/td&gt;
&lt;td align="right"&gt;2.3&lt;/td&gt;
&lt;td align="right"&gt;2.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Python&lt;/td&gt;
&lt;td&gt;2.7.10&lt;/td&gt;
&lt;td align="right"&gt;1014&lt;/td&gt;
&lt;td align="right"&gt;1014&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PyPy&lt;/td&gt;
&lt;td&gt;2.6.1&lt;/td&gt;
&lt;td align="right"&gt;96&lt;/td&gt;
&lt;td align="right"&gt;96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julia&lt;/td&gt;
&lt;td&gt;0.4.0-rc3&lt;/td&gt;
&lt;td align="right"&gt;87&lt;/td&gt;
&lt;td align="right"&gt;6.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cython&lt;/td&gt;
&lt;td&gt;0.23.3&lt;/td&gt;
&lt;td align="right"&gt;729&lt;/td&gt;
&lt;td align="right"&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Numba&lt;/td&gt;
&lt;td&gt;0.21.0&lt;/td&gt;
&lt;td align="right"&gt;867&lt;/td&gt;
&lt;td align="right"&gt;47&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;
Times are all in μs/atom. System size is 4000 atoms.
Hardware is Xeon E5-2630 v3 @ 2.4 Ghz, OS is Ubuntu 12.04.
&lt;br&gt;
The 'Initial Time' column results from the minimal amount of code changes to get the compiler working.
&lt;br&gt;
The 'Final Time' is the time after the tuning in this post.&lt;/p&gt;
&lt;h3&gt;Julia&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;cell&lt;/em&gt; version contains the same issue with array operations as the &lt;em&gt;nsquared&lt;/em&gt; version - the computation of &lt;code&gt;dd&lt;/code&gt; allocates a temporary to hold the results every time through the loop.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/lindahua/Devectorize.jl"&gt;Devectorize&lt;/a&gt; package can automatically convert array notation
to a loop.  If we add the &lt;code&gt;@devec&lt;/code&gt; annotation, the time drops to 43 μs/atom.
Unfortunately, the allocation to hold the result must still be performed, and it remains inside the inner particle loop.
If we manually create the loop and hoist the allocation out of the loop, time is 27 μs/atom.&lt;/p&gt;
&lt;p&gt;The code uses &lt;code&gt;dot&lt;/code&gt; to compute the vector norm.  This calls a routine (&lt;code&gt;julia_dot&lt;/code&gt;) to perform the
dot product.
For long vectors calling an optimized linear algebra routine is beneficial, but for a vector of length 3 this adds overhead.
Replacing &lt;code&gt;dot&lt;/code&gt; with the equivalent loop reduces the time to 23 μs/atom.&lt;/p&gt;
&lt;p&gt;Looking through the memory allocation output (&lt;code&gt;--track-allocation=user&lt;/code&gt;) shows some vector operations
when the force array is zeroed and accumulated.
Also in &lt;code&gt;putAtomInBox&lt;/code&gt; in &lt;code&gt;linkcell.jl&lt;/code&gt;.
 These spots are also visible in the profile output, but the profile output is less convenient because it is not shown with source.
The &lt;code&gt;@devec&lt;/code&gt; macro does work here, and the performance is now 7.7 μs/atom.   Explicit loops
give a slightly better time of 7.3 μs/atom.&lt;/p&gt;
&lt;p&gt;Profiling shows even more opportunities for devectorization in &lt;code&gt;advanceVelocity&lt;/code&gt; and &lt;code&gt;advancePosition&lt;/code&gt; in &lt;code&gt;simflat.jl&lt;/code&gt;  Time is now 6.4 μs/atom.&lt;/p&gt;
&lt;p&gt;The Julia executable has a &lt;code&gt;-O&lt;/code&gt; switch for more time-intensive optimizations (it adds more LLVM optimization passes).   This improves the time to 6.2 μs/atom.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;@fastmath&lt;/code&gt; macro improves the time a little more, to 6.1 μs/atom.
The &lt;code&gt;@inbounds&lt;/code&gt; macro to skip the bounds checks did not seem to improve the time.&lt;/p&gt;
&lt;p&gt;The final Julia time is now within a factor of 3 of the C time.  The code is &lt;a href="https://gist.github.com/markdewing/54709a0fd6a17348a7cb"&gt;here&lt;/a&gt;.  It's not clear where the remaining time overhead comes from. &lt;/p&gt;
&lt;h3&gt;PyPy&lt;/h3&gt;
&lt;p&gt;The PyPy approach to JIT compilation is very general, but that also makes it difficult to target what code
changes might improve performance.
The &lt;a href="https://bitbucket.org/pypy/jitviewer"&gt;Jitviewer&lt;/a&gt; tool is nice, but not helpful at a cursory glance.
The &lt;a href="https://vmprof.readthedocs.org/en/latest/"&gt;vmprof&lt;/a&gt; profiler solves an important problem by collecting the native code stack plus the python stack. 
In this particular case, it reports at the function level, and the bulk of the time was spent in &lt;code&gt;computeForce&lt;/code&gt;.
I hope to write more about vmprof in a future post, as it could help with integrated profiling of Python + native code (either compiled or JIT-ed).&lt;/p&gt;
&lt;h3&gt;Cython&lt;/h3&gt;
&lt;p&gt;The simplest step is to add an initialization line and move some &lt;code&gt;.py&lt;/code&gt; files to &lt;code&gt;.pyx&lt;/code&gt; files.  This gives 729 μs/atom.
Adding types to the computeForce function and assigning a few attribute lookups to local variables so the types can be assigned (playing a game of 'remove the yellow' in the Cython annotation output) gives 30 μs/atom.&lt;/p&gt;
&lt;p&gt;Adding types and removing bounds checks more routines  (in  &lt;code&gt;halo.py&lt;/code&gt;, &lt;code&gt;linkcell.py&lt;/code&gt;, &lt;code&gt;simflat.py&lt;/code&gt;) gives 13 μs/atom.&lt;/p&gt;
&lt;p&gt;Code is &lt;a href="https://gist.github.com/markdewing/3688c6eebc0a88081e07"&gt;here&lt;/a&gt;.
Further progress needs deeper investigation with profiling tools.&lt;/p&gt;
&lt;h3&gt;Numba&lt;/h3&gt;
&lt;p&gt;Starting with adding &lt;code&gt;@numba.jit&lt;/code&gt; decorators to &lt;code&gt;computeForce&lt;/code&gt;, and the functions it calls gives the
initial time of 867 μs/atom.
Extracting all the attribute lookups (including the function call to &lt;code&gt;getNeighborBoxes&lt;/code&gt;) gives 722 μs/atom.&lt;/p&gt;
&lt;p&gt;We should ensure the call to &lt;code&gt;getNeighborBoxes&lt;/code&gt; is properly JIT-ed.  Unfortunately, this requires more involved
code restructuring.  Functions need to be split into a wrapper that performs any needed attribute lookups, and
an inner function that gets JIT-ed.  Loop lifting automatically performs this transformation on functions
  with loops.  On functions without loops, however, it needs to be done manually.
Once this is done, the time improves dramatically to 47 μs/atom.&lt;/p&gt;
&lt;p&gt;Hopefully the upcoming "JIT Classes" feature will make this easier, and require less code restructuring. &lt;/p&gt;
&lt;p&gt;Code is &lt;a href="https://gist.github.com/markdewing/89cce577f5b8625cc776"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;Julia is leading in terms of getting the best performance on this example.  Many of these projects are rapidly improving, so this is just a snapshot at their current state.&lt;/p&gt;
&lt;p&gt;All these projects need better profiling tools to show the user where code is slow and to give feedback on why the code is slow.
The Cython annotated output is probably the the best - it highlights which lines need attention. 
However it is not integrated with profiler output, so in a project of any size, it's not clear where a user should spend time adding types.  &lt;/p&gt;
&lt;p&gt;Julia has some useful collection and feedback tools, but they would be much more helpful if combined.  The memory allocation output is bytes allocated.
It's useful for finding allocations where none were expected, or for allocations in known hot loops, but it's less clear which other allocations are impacting performance.
Ideally this could be integrated with profiler output and weighted by time spent to show which allocations are actually affecting execution time.&lt;/p&gt;&lt;/div&gt;</description><category>CoMD</category><category>cython</category><category>julia</category><category>Numba</category><category>PyPy</category><category>python</category><guid>https://markdewing.github.io/blog/posts/improvements-in-comd-cell-method-performance/</guid><pubDate>Fri, 02 Oct 2015 18:56:00 GMT</pubDate></item><item><title>Two Meanings of Vectorization</title><link>https://markdewing.github.io/blog/posts/two-meanings-of-vectorization/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;The term 'vectorize' as used by programmers has at least two separate uses.
Both uses can have implications for performance, which sometimes leads to confusion.&lt;/p&gt;
&lt;p&gt;One meaning refers to a language syntax to express operations on multiple values - typically an entire array, or a slice of a array.
This can be a very convenient notation for expressing algorithms.&lt;/p&gt;
&lt;p&gt;Here is a simple example (in Julia) using loop-oriented (devectorized) notation&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;Float64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;Float64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# allocate space for result&lt;/span&gt;
&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;Float64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;
    &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Now compare with using vectorized (array) notation&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;Float64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;Float64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# space for result automatically allocated&lt;/span&gt;

&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;The vectorized notation is more compact.
Julia and Python/Numpy programmers usually mean this when referring to 'vectorization'.
See more in the Wikipedia entry on &lt;a href="https://en.wikipedia.org/wiki/Array_programming"&gt;Array programming&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;John Myles White wrote a post discussing the performance implications of &lt;a href="http://www.johnmyleswhite.com/notebook/2013/12/22/the-relationship-between-vectorized-and-devectorized-code/"&gt;vectorized and devectorized code&lt;/a&gt; in Julia and R.
Note that Python/Numpy operates similar to R as described in the post - good performance usually requires appropriately vectorized code, because that skips the interpreter and calls higher performing C routines underneath.&lt;/p&gt;
&lt;p&gt;The other meaning of 'vectorization' refers to generating assembly code to make effective use of fine-grained parallelism in hardware SIMD units.
This is what Fortran or C/C++ programmers (and their compilers) mean by 'vectorization'.
In Julia, the &lt;code&gt;@simd&lt;/code&gt; macro gives hints to the compiler that a given loop can be vectorized.&lt;/p&gt;
&lt;p&gt;See more in the Wikipedia entry on &lt;a href="https://en.wikipedia.org/wiki/Automatic_vectorization"&gt;Automatic vectorization&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>vectorization</category><guid>https://markdewing.github.io/blog/posts/two-meanings-of-vectorization/</guid><pubDate>Fri, 02 Oct 2015 03:23:00 GMT</pubDate></item><item><title>Why Types Help Performance</title><link>https://markdewing.github.io/blog/posts/why-types-help-performance/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;In &lt;a href="http://markdewing.github.io/blog/posts/first-performance-improvements"&gt;previous&lt;/a&gt; &lt;a href="http://markdewing.github.io/blog/posts/comparing-languages-with-miniapps/"&gt;posts&lt;/a&gt;, we've seen that adding type information can help the performance of the code generated by dynamic language compilers.
The documentation for Cython annotations talks of 'Python interaction', and Numba has 'object' mode and 'nopython' modes.
In post I will look more at what these mean, and how they affect performance.&lt;/p&gt;
&lt;p&gt;To start, consider how values are represented in a computer, such as a simple integer ('int' type in C).
The bare value takes 4 bytes in memory, and no additional information about it is stored,
such as its type or how much space it occupies.
This information is all implicit at run time &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://markdewing.github.io/blog/posts/why-types-help-performance/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;.
That it takes 4-bytes and is interpreted as an integer is determined at compile time.&lt;/p&gt;
&lt;p&gt;In dynamic languages, this extra information can be queried at run-time.
For example:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt; &lt;span class="s"&gt;'int'&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;The value stored in &lt;code&gt;a&lt;/code&gt; has type &lt;code&gt;int&lt;/code&gt;, Python's integer type.
This extra information must be stored somewhere, and languages often solve this by wrapping
the bare value in an object.
This is usually called 'boxing'.
The value plus type information (and any other information) is called a 'boxed type'.
The bare value is called an 'unboxed type' or a 'primitive value'.&lt;/p&gt;
&lt;p&gt;In Java, different types can be explicitly created (&lt;code&gt;int&lt;/code&gt; vs. &lt;code&gt;Integer&lt;/code&gt;), and the programmer
needs to know the differences and tradeoffs.
(See this &lt;a href="http://stackoverflow.com/questions/13055/what-is-boxing-and-unboxing-and-what-are-the-trade-offs"&gt;Stack Overflow question&lt;/a&gt; for more about boxed and primitive types.)&lt;/p&gt;
&lt;p&gt;Python only has boxed values ('everything is an object'). From the interpreter, this means we can
always determine the type of a value.
If we look a layer down, as would be needed to integrate with C, these values are accessed through the Python API.
The base type of any object is PyObject.  For our simple example, integers are stored as PyIntObject.&lt;/p&gt;
&lt;p&gt;For example, consider the following Python code.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;One way to see what calls the interpreter would make is to compile with Cython.
The following C is the result (simplified - reference counting pieces to the Python API are omitted.)&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;PyObject&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nf"&gt;add&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;PyObject&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;pyx_int_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PyInt_FromLong&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;PyObject&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;pyx_int_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PyInt_FromLong&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;PyObject&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;pyx_tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PyNumber_Add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pyx_int_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pyx_int_2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pyx_tmp&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Without type information, Cython basically unrolls the interpreter loop, and makes
a sequence of Python API calls.
The HTML annotation output highlights lines with Python interaction, and can be expanded to show
the generated code.   This gives feedback on where and what types need to be added to avoid
the Python API calls.&lt;/p&gt;
&lt;p&gt;Add some Cython annotations and the example becomes&lt;/p&gt;
&lt;pre class="code literal-block"&gt; &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;
    &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Now the following code is generated&lt;/p&gt;
&lt;pre class="code literal-block"&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;add&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
 &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
 &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;The add of the two integers is done directly (and runs much faster), rather than going through the Python API call.&lt;/p&gt;
&lt;h3&gt;Numba&lt;/h3&gt;
&lt;p&gt;If its type information is insufficient, Numba will call the Python API for every operation.
Since all the operations occur on Python objects, this is called 'object' mode (slow).
With sufficient type information, code can be generated with no calls to the Python API, and hence
the name 'nopython' mode (fast).&lt;/p&gt;
&lt;h3&gt;Julia&lt;/h3&gt;
&lt;p&gt;Julia has boxed object types, but is designed to try use the unboxed types as much as possible.
The most generic type is called 'Any', and it is possible to produce Julia code that runs this mode.&lt;br&gt;
See the section on &lt;a href="http://julia.readthedocs.org/en/latest/manual/embedding/"&gt;Embedding Julia&lt;/a&gt; in the
documentation for more about Julia objects.&lt;/p&gt;
&lt;p&gt;Julia's type inference only happens inside functions.
This is why composite types (structures) need type annotations for good performance.&lt;/p&gt;
&lt;p&gt;This example demonstrates the issue&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;type&lt;/span&gt;&lt;span class="nc"&gt; Example&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="nf"&gt; add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;val&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="n"&gt;v1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Example&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;v2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Example&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="n"&gt;code_llvm&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c"&gt;# The @code_llvm macro prints the LLVM IR.  &lt;/span&gt;
&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"res = "&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Since the type of the 'val' element is not known, the code operates on a generic object type &lt;code&gt;jl_value_t&lt;/code&gt; and eventually calls &lt;code&gt;jl_apply_generic&lt;/code&gt;, which looks up the right method and dispatches to it at execution time.
(The LLVM IR is not shown here - run the example to see it.)  Doing all this at execution time is slow.&lt;/p&gt;
&lt;p&gt;Now add the type annotation&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;type&lt;/span&gt;&lt;span class="nc"&gt; Example&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;Int&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;The resulting LLVM IR (also not shown here) is much shorter in that it adds two integers directly and
returns the result as an integer.
With type information, the lookup and dispatch decisions can be made at compile time.&lt;/p&gt;
&lt;p&gt;Note that Julia uses a Just In Time (JIT) compiler, which means compilation occurs at run time.
The run time can be split into various phases, which include compilation and execution of
the resulting code.&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;Hopefully this post sheds some light on how type information can affect the performance of dynamic languages.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;The type can be accessible at run time via debug information.  See this Strange Loop 2014 talk: &lt;a href="https://www.youtube.com/watch?v=LwicN2u6Dro"&gt;Liberating the lurking Smalltalk in Unix&lt;/a&gt; &lt;a class="footnote-backref" href="https://markdewing.github.io/blog/posts/why-types-help-performance/#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>cython</category><category>julia</category><category>Numba</category><category>python</category><guid>https://markdewing.github.io/blog/posts/why-types-help-performance/</guid><pubDate>Thu, 24 Sep 2015 15:48:00 GMT</pubDate></item><item><title>First Performance Improvements</title><link>https://markdewing.github.io/blog/posts/first-performance-improvements/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;The &lt;a href="http://markdewing.github.io/blog/posts/comparing-languages-with-miniapps/"&gt;previous post&lt;/a&gt; introduced the CoMD miniapp in Python and Julia.
This post will look a little deeper into the performance of Julia and various Python compilers.
I will use the nsquared version of the code for simplicity.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;computeForce&lt;/code&gt; function in &lt;code&gt;ljforce&lt;/code&gt; takes almost all the time, and it is here we should focus.
In the nsquared version there are no further function calls inside this loop, which should make it
easier to analyze and improve the performance.&lt;/p&gt;
&lt;p&gt;The summary performance table (all times are in microseconds/atom, the system is the smallest size at 256 atoms)&lt;/p&gt;
&lt;!-- from laptop --&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language/compiler  &lt;/th&gt;
&lt;th&gt;version   &lt;/th&gt;
&lt;th align="right"&gt;Initial time&lt;/th&gt;
&lt;th align="right"&gt;  Final time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Python&lt;/td&gt;
&lt;td&gt;2.7.10&lt;/td&gt;
&lt;td align="right"&gt;560&lt;/td&gt;
&lt;td align="right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PyPy&lt;/td&gt;
&lt;td&gt;2.6.1&lt;/td&gt;
&lt;td align="right"&gt;98&lt;/td&gt;
&lt;td align="right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HOPE&lt;/td&gt;
&lt;td&gt;0.4.0&lt;/td&gt;
&lt;td align="right"&gt;8&lt;/td&gt;
&lt;td align="right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cython&lt;/td&gt;
&lt;td&gt;0.23.1&lt;/td&gt;
&lt;td align="right"&gt;335&lt;/td&gt;
&lt;td align="right"&gt;8.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Numba&lt;/td&gt;
&lt;td&gt;0.21.0&lt;/td&gt;
&lt;td align="right"&gt;450&lt;/td&gt;
&lt;td align="right"&gt;8.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julia&lt;/td&gt;
&lt;td&gt;0.5.0-dev+50&lt;/td&gt;
&lt;td align="right"&gt;44&lt;/td&gt;
&lt;td align="right"&gt;7.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- From desktop 
| Python            | 2.7.10  | 438          |            |
| PyPy              |         |  84          |            |
| HOPE              |         |   7          |            |
| Cython            |         | 329          |  49        |
| Numba             |         | 405          |   7        |
| Julia             |         |  45          |  19        |
--&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;(This table uses a different code version and different hardware, so the numbers are not comparable to the previous post)
&lt;br&gt;
(Hardware is i7-3720QM @ 2.6 Ghz and OS is Ubuntu 14.04)
&lt;br&gt;
The 'Initial Time' column results from the minimal amount of code changes to get the compiler working.
&lt;br&gt;
The 'Final Time' is the time after doing some tuning (for this post, anyway - there's still more work to be done).&lt;/p&gt;
&lt;!--The level of tuning in this post is about adding type information and removing temporary memory allocations.
Further investigation of the generated intermediate and assembly code is yet to be done.
--&gt;

&lt;h3&gt;HOPE&lt;/h3&gt;
&lt;p&gt;The HOPE compiler will compile the nsquared version, provided the branch involving the cutoff is modified to avoid the &lt;code&gt;continue&lt;/code&gt; statement.
The loop that zeros the force also needs a simple name change to the loop variable.
The backend C++ compiler is gcc 4.8.4.  No attempt was made to try different compilers or further optimize the generated code.&lt;/p&gt;
&lt;h3&gt;Numba&lt;/h3&gt;
&lt;p&gt;To use Numba, we need to add &lt;code&gt;import numba&lt;/code&gt; to &lt;code&gt;ljforce.py&lt;/code&gt; and add the &lt;code&gt;@numba.jit&lt;/code&gt; decorator to the &lt;code&gt;computeForce&lt;/code&gt; method.
This gives the time in the initial column (450 μs/atom) , which is about a 20% improvement over the plain Python version.&lt;/p&gt;
&lt;p&gt;Code involving attribute lookups cannot currently be compiled to efficient code, and these lookups occur inside this inner loop.
And the compiler will not hoist attribute lookups outside the loop.
This can be done manually by assigning the attribute to a temporary variable before the loop, and replacing the values in the loop body. 
This transformation enables effective compilation of the loop.&lt;/p&gt;
&lt;p&gt;(Internally Numba performs loop-lifting, where it extracts the loop to a separate function in order to compile the loop.)&lt;/p&gt;
&lt;!--Loop-lifting is a way for Numba to extract a loop into a separate function in order to compile the loop.--&gt;

&lt;p&gt;The beginning of the function now looks like&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    &lt;span class="nd"&gt;@numba.jit&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;computeForce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sim&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c"&gt;# hoist the attribute lookups of these values&lt;/span&gt;
        &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;
        &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;
        &lt;span class="n"&gt;bounds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bounds&lt;/span&gt;
        &lt;span class="n"&gt;nAtoms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nAtoms&lt;/span&gt;
        &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;

        &lt;span class="c"&gt;# rest of original function&lt;/span&gt;
        &lt;span class="c"&gt;# ...&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Now Numba can compile the time consuming loop, and this gives about 9 μs/atom.&lt;/p&gt;
&lt;p&gt;The loop that zeros the force can be slightly improved by either looping over the last dimension
explicitly, or by zeroing the entire array at once.  This change yields the final number in the table (8.3 μs/atom).&lt;/p&gt;
&lt;p&gt;The whole modified file is &lt;a href="https://gist.github.com/markdewing/eb0bf52ea1b71995150a"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Cython&lt;/h3&gt;
&lt;p&gt;The simplest change to enable Cython is to move &lt;code&gt;ljforce.py&lt;/code&gt; to &lt;code&gt;ljforce.pyx&lt;/code&gt;, and add &lt;code&gt;import pyximport; pyximport.install()&lt;/code&gt; to the beginning of &lt;code&gt;simflat.py&lt;/code&gt;.
This initial time (335 μs/atom) gives a 40% improvement over regular python, but there is more performance available.&lt;/p&gt;
&lt;p&gt;The first step is to add some type information.
In order to do this we need to hoist the attribute lookups and assign to temporary variables, as in the Numba version.
At this step, we add types for the same variables as the Numba version.
The beginning of the function looks like this:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;computeForce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="n"&gt;nAtoms&lt;/span&gt;
        &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="n"&gt;double&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;
        &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;
        &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;
        &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double_t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;bounds&lt;/span&gt;

        &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;
        &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;
        &lt;span class="n"&gt;bounds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bounds&lt;/span&gt;
        &lt;span class="n"&gt;nAtoms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nAtoms&lt;/span&gt;
        &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;

        &lt;span class="c"&gt;# rest of original function&lt;/span&gt;
        &lt;span class="c"&gt;# ...&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;The time for this change about 54 μs/atom.&lt;/p&gt;
&lt;p&gt;Cython has a convenient feature that creates an annotated HTML file highlighting lines in the 
original file that may causing a performance issue.  Run &lt;code&gt;cython -a ljforce.pyx&lt;/code&gt; to get the report.
This indicates some more type declarations need to be added.
Adding these types gives about 8.6 μs/atom.   Finally a decorator can be added to remove bounds checks (&lt;code&gt;@cython.boundscheck(False)&lt;/code&gt;) to get the final performance in the table (8.3 μs/atom).&lt;/p&gt;
&lt;p&gt;The whole &lt;code&gt;ljforce.pyx&lt;/code&gt; file is &lt;a href="https://gist.github.com/markdewing/7017e23c883a2bd297cb"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Julia&lt;/h3&gt;
&lt;p&gt;The biggest issue in this code seems to be allocations inside the inner loop.
The memory performance tools can indicate where unexpected allocations are occurring.
One tool is to use a command line option (&lt;code&gt;--track-allocation=user&lt;/code&gt;) to the julia executable.&lt;/p&gt;
&lt;p&gt;One problem is a temporary created inside the loop to hold the results of an array operation (the line that computes &lt;code&gt;dd&lt;/code&gt;).
Moving this allocation out of the loop and setting each element separately improves performance 
to 19 μs/atom.
Another allocation occurs when updating the force array using a slice.  Changing this to explicitly loop
over the elements improves the performance to the final numbers shown in the table (7.1 μs/atom).&lt;/p&gt;
&lt;p&gt;The final modified file is &lt;a href="https://gist.github.com/markdewing/f1c9a46a2fec8a3ade4f"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;The performance numbers have quite a bit of variance, and they are not a result of a rigorous benchmarking and statistics collection.  If you want to compare between compilers, the final results should probably be read something like: "The performance of Cython and Numba is roughly the same on this code, and Julia is a little bit faster for this code".
Also keep in mind we're not done yet digging into the performance of these different compilers.&lt;/p&gt;
&lt;p&gt;Some simple changes to the code can give dramatic performance improvements, but the difficulty is
discovering what changes need to be made and where to make them.&lt;/p&gt;
&lt;p&gt;Future topics to explore:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply these lessons to the cell version of the code.&lt;/li&gt;
&lt;li&gt;With Julia and Numba, it's hard to connect intermediate code stages (internal IR, LLVM IR, assembly) to the original code, and to spot potential performance issues there.  The Cython annotation output is nice here.&lt;/li&gt;
&lt;li&gt;The difference between operating on dynamic objects versus the underlying value types.&lt;/li&gt;
&lt;li&gt;How well does the final assembly utilize the hardware. How to use hardware sampling for analysis.&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>cython</category><category>julia</category><category>Numba</category><category>PyPy</category><category>python</category><guid>https://markdewing.github.io/blog/posts/first-performance-improvements/</guid><pubDate>Fri, 18 Sep 2015 04:48:00 GMT</pubDate></item><item><title>Comparing languages with miniapps</title><link>https://markdewing.github.io/blog/posts/comparing-languages-with-miniapps/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;The &lt;a href="https://mantevo.org/"&gt;Mantevo project&lt;/a&gt; provides a collection of miniapps, which are simplified versions
of real scientific applications that make it easier to explore performance, scaling, languages, programming models, etc.
I want to focus on the language aspect and port some apps to new languages to see how they compare. &lt;/p&gt;
&lt;p&gt;The first miniapp I started with is &lt;a href="http://www.exmatex.org/comd.html"&gt;CoMD&lt;/a&gt;, a molecular dynamics code in C.&lt;/p&gt;
&lt;p&gt;For the language ports, I made multiple variants of increasing complexity.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;nsquared&lt;/dt&gt;
&lt;dd&gt;This uses the naive algorithm in computing inter-particle interactions. The central loop computes the
   interaction of every particle with every other particle.  The scaling of run time vs number of particles is N&lt;sup&gt;2&lt;/sup&gt;.&lt;/dd&gt;
&lt;dt&gt;cell&lt;/dt&gt;
&lt;dd&gt;The cell list method divides space into cells and tracks the particles in each cell.  When computing interactions, only the particles in neighboring cells need to be considered.  The scaling of run time vs. the number of particles is N.&lt;/dd&gt;
&lt;dt&gt;mpi&lt;/dt&gt;
&lt;dd&gt;Parallel version of the cell method.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;The C version corresponds to the 'cell' and 'mpi' variants (plus the C version has OpenMP and several other programming model variants)&lt;/p&gt;
&lt;p&gt;Currently there are Python and Julia ports for the nsquared and cell variants, and a Python version of the mpi variant.
They are available in my 'multitevo' github repository: &lt;a href="https://github.com/markdewing/multitevo"&gt;https://github.com/markdewing/multitevo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Julia version is a pretty straightforward port of the Python version, so it is probably not very idiomatic Julia code.
(I would be happy to take suggestions from the Julia community on how to improve the style and organization)&lt;/p&gt;
&lt;!--
C does not have multidimensional arrays so the 3D cell indices need to be mapped to a linear index of atom coordinates (and
the reverse), and this involves some complicated expressions.
The Julia and Python version preserve this approach, but since they have multidimensional arrays,
it might make the code much simpler to store the atom information directly in a multidimensional array.
--&gt;

&lt;h3&gt;Scaling with system size&lt;/h3&gt;
&lt;p&gt;First let's verify the scaling of the nsquared version vs. the cell list version (using the Julia versions).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Graph of scaling nsquared and cell list method" src="https://markdewing.github.io/blog/2015/scaling.png"&gt;&lt;/p&gt;
&lt;p&gt;As expected, the cell list variant has better scaling at larger system sizes.&lt;/p&gt;
&lt;!--
Data for nsquared variant
natoms time(s)   time_per_atom (us/atom)
256   0.011350  44.335089
500   0.034140  68.280823
864   0.099966 115.700927
1372   0.239239 174.372460
2048   0.509494 248.776288
2916   1.050071 360.106739
4000   1.942931 485.732711
5324   3.327456 624.991804
6912   5.769225 834.667932
--&gt;

&lt;!--
Data for cell method variant
natoms  time(s)   time_per_atom (us/atom)
256   0.045821 178.987427
500   0.049810  99.619442
864   0.142849 165.333940
1372   0.151120 110.145543
2048   0.310324 151.525439
2916   0.344807 118.246625
4000   0.344878  86.219497
5324   0.621829 116.797365
6912   0.626944  90.703648
--&gt;

&lt;h3&gt;Initial performance&lt;/h3&gt;
&lt;p&gt;For a purely computational code such as this, performance matters.
The ultimate goal is near C/Fortran speeds using a higher-level language to express the algorithm.&lt;/p&gt;
&lt;p&gt;Some initial timings (for a system size of 4000 atoms, using the cell variant)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language/Compiler  &lt;/th&gt;
&lt;th align="left"&gt;Version&lt;/th&gt;
&lt;th align="right"&gt;Time/atom (microseconds)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;C - gcc&lt;/td&gt;
&lt;td align="left"&gt;4.8.2&lt;/td&gt;
&lt;td align="right"&gt;2.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julia&lt;/td&gt;
&lt;td align="left"&gt;0.3.11&lt;/td&gt;
&lt;td align="right"&gt;153.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julia&lt;/td&gt;
&lt;td align="left"&gt;0.4.0-dev+6990&lt;/td&gt;
&lt;td align="right"&gt;88.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Python&lt;/td&gt;
&lt;td align="left"&gt;2.7.10 (from Anaconda)  &lt;/td&gt;
&lt;td align="right"&gt;941.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Numba&lt;/td&gt;
&lt;td align="left"&gt;0.20.0&lt;/td&gt;
&lt;td align="right"&gt;789.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pypy&lt;/td&gt;
&lt;td align="left"&gt;2.6.1&lt;/td&gt;
&lt;td align="right"&gt;98.4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;(Hardware is Xeon E5-2630 v3 @ 2.4 Ghz, OS is Ubuntu 12.04)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: These numbers indicate how a particular version of the language and compiler perform on a particular version of this code. The main purpose for these numbers is a baseline to measure future performance improvements.&lt;/p&gt;
&lt;p&gt;I tried &lt;a href="http://www.cosmology.ethz.ch/research/software-lab/HOPE.html"&gt;HOPE&lt;/a&gt;, a Python to C++ JIT compiler.
It require some modifications to the python code, but then failed in compiling the resulting C++ code.
I also tried &lt;a href="http://www.parakeetpython.com/"&gt;Parakeet&lt;/a&gt;.  It failed to translate the Python code, and I did not investigate further.&lt;/p&gt;
&lt;p&gt;It is clear when comparing to C there is quite a bit of room for improvement in the code using the high-level language compilers (Julia, Numba, PyPy).
Whether that needs to come from the modifications to the code, or improvements in the compilers, I don't know yet.&lt;/p&gt;
&lt;p&gt;The only real performance optimization so far has been adding type declarations to the composite types in Julia.
This boosted performance by about 3x. Without the type declarations, the Julia 0.4.0 speed is about 275 us/atom.
First performance lesson: Add type declarations to composite types in Julia.&lt;/p&gt;
&lt;p&gt;Julia and Numba have a number of similarities and so I want to focus on improving the performance of the code
under these two systems in the next few posts.&lt;/p&gt;
&lt;!--
Julia and Numba have similar stages (infer types, convert to LLVM IR, convert to native code), and so I hope to look
at them in parallel going forward.  
--&gt;&lt;/div&gt;</description><category>CoMD</category><category>julia</category><category>Mantevo</category><category>Numba</category><category>PyPy</category><category>python</category><guid>https://markdewing.github.io/blog/posts/comparing-languages-with-miniapps/</guid><pubDate>Wed, 02 Sep 2015 16:18:50 GMT</pubDate></item><item><title>Running Programs on Epiphany</title><link>https://markdewing.github.io/blog/posts/running-programs-on-epiphany/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;In the &lt;a href="https://markdewing.github.io/blog/posts/communicating-with-epiphany/index.html"&gt;last post&lt;/a&gt;, we looked at the memory layout and ran a simple demo.
Let's take a closer look at how a program starts running.&lt;/p&gt;
&lt;p&gt;For starters, compile and run the program from last time.  We're going to run the program again, this time manually using the low-level mechanism that &lt;code&gt;e-loader&lt;/code&gt; uses to start programs.&lt;/p&gt;
&lt;p&gt;Writing a 1 (SYNC) value to the ILATST register will trigger the Sync interrupt, and cause an existing program to run on the core.&lt;/p&gt;
&lt;p&gt;Remember that the registers from each core are memory mapped to the host.
The register addresses are defined in &lt;code&gt;/opt/adapteva/esdk/tools/host/include/epiphany-hal-data.h&lt;/code&gt;.
We want E_REG_ILATST, which is 0xf042C.&lt;/p&gt;
&lt;p&gt;To see if this works, change the input values.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;parallella@parallella:~$ e-write 0 0 2000 4 5 0
[0x00002000] = 0x00000004
[0x00002004] = 0x00000005
[0x00002008] = 0x00000000
&lt;/pre&gt;


&lt;p&gt;Now, write the ILATST register to run the program&lt;/p&gt;
&lt;pre class="code literal-block"&gt;parallella@parallella:~$ e-write 0 0 0xf042c 1
&lt;/pre&gt;


&lt;p&gt;And check the result&lt;/p&gt;
&lt;pre class="code literal-block"&gt;parallella@parallella:~$ e-read 0 0 0x2000 3
[0x00002000] = 0x00000004
[0x00002004] = 0x00000005
[0x00002008] = 0x00000009
&lt;/pre&gt;


&lt;p&gt;It worked!&lt;/p&gt;
&lt;p&gt;Okay, now let's look at the details of what happened.&lt;/p&gt;
&lt;p&gt;The first 40 bytes of the core-local memory are reserved for the Interrupt Vector Table (IVT).
Each entry is 4 bytes long, and should contain a jump (branch) instruction to the desired code.
The first entry in the table is the Sync interrupt, used to start the program.
(See the &lt;a href="http://adapteva.com/docs/epiphany_arch_ref.pdf"&gt;Epiphany Architecture Reference&lt;/a&gt; for the rest of the IVT entries)&lt;/p&gt;
&lt;p&gt;We can disassemble the compiled object file with &lt;code&gt;e-objdump -D&lt;/code&gt; and look for address 0 
(we need -D instead of -d to disassemble all the sections, not just the normal executable sections).&lt;/p&gt;
&lt;p&gt;This looks promising.  Address of 0, in a section called &lt;code&gt;ivt_reset&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Disassembly of section ivt_reset:

00000000 &amp;lt;_start&amp;gt;:
   0:   2ce8 0000       b 58 &amp;lt;.normal_start&amp;gt;
&lt;/pre&gt;


&lt;p&gt;After the Sync interrupt, control transfers to address 0, and then branches to location 0x58.
The next section in the objdump output has that address.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Disassembly of section .reserved_crt0:

00000058 &amp;lt;.normal_start&amp;gt;:
  58:   720b 0002       mov r3,0x90
  5c:   600b 1002       movt r3,0x0
  60:   0d52            jalr r3
&lt;/pre&gt;


&lt;p&gt;This loads address 0x90 and jumps to it.
The &lt;code&gt;mov&lt;/code&gt; instruction loads the lower 16 bits of r3 and the &lt;code&gt;movt&lt;/code&gt; instruction loads the upper 16 bits.&lt;/p&gt;
&lt;p&gt;Now look for that address&lt;/p&gt;
&lt;pre class="code literal-block"&gt;Disassembly of section .text:

00000090 &amp;lt;_epiphany_start&amp;gt;:
  90:   be0b 27f2       mov sp,0x7ff0
  94:   a00b 3002       movt sp,0x0

   ... 
&lt;/pre&gt;


&lt;p&gt;This sets up the stack by loading the stack pointer with 0x7ff0, which is the top of the 32K local address space.
The code calls other routines, which eventually call &lt;code&gt;main&lt;/code&gt;, but I won't trace it all here.&lt;/p&gt;&lt;/div&gt;</description><category>epiphany</category><category>parallella</category><guid>https://markdewing.github.io/blog/posts/running-programs-on-epiphany/</guid><pubDate>Tue, 01 Sep 2015 18:13:00 GMT</pubDate></item><item><title>Communicating with Epiphany</title><link>https://markdewing.github.io/blog/posts/communicating-with-epiphany/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;This post is going to look at the Epiphany memory map, and give a very simple example demonstration.
I will skim over some background information that is covered elsewhere.
See the following posts and resources that describe the Parallella and Epiphany architectures.&lt;/p&gt;
&lt;h3&gt;Resources&lt;/h3&gt;
&lt;p&gt;Parallella Chronicles&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.parallella.org/2014/11/25/parallella-chronicles-part-one-2/"&gt;Part One: Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.parallella.org/2014/12/15/parallella-chronicles-part-two-2/"&gt;Part Two: The Software Development Kit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.parallella.org/2015/01/14/parallella-chronicles-part-three/"&gt;Part Three: "Hello World"&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.parallella.org/2015/02/28/parallella-chronicles-part-five/"&gt;Part Five: The Epiphany Memory Map&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Technical Musings&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://suzannejmatthews.github.io/2015/06/02/epiphany-overview/"&gt;Overview of Epiphany Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://suzannejmatthews.github.io/2015/06/03/epiphany-hello-world/"&gt;Hello Epiphany&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Manuals&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://adapteva.com/docs/epiphany_arch_ref.pdf"&gt;Epiphany Architecture Reference Manual&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://adapteva.com/docs/epiphany_sdk_ref.pdf"&gt;Epiphany SDK Reference Manual&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Source code&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/adapteva/epiphany-libs"&gt;epiphany-libs&lt;/a&gt; repo on github with the e-hal and various utilities.
The epiphany-sdk repo contains download and build script for the GNU toolchain.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This post is going to be written from the perspective of a PC programmer.
Desktop operating systems use virtual memory, and programmers don't have to think about hardware addresses very much. 
The relative addresses inside each process matter most.
Many of the addresses on the Epiphany are fixed, or fixed relative to each core, and require more 'hard-coding' of addresses.
Although most of that is accomplished through the toolchain, it is useful to understand when programming the board.
(Programmers of embedded systems are more used to this sort of memory layout control.)&lt;/p&gt;
&lt;h3&gt;Memory Layout&lt;/h3&gt;
&lt;p&gt;The Epiphany contains onboard RAM (32K per core). This called 'local' or 'core-local' memory, and is the fastest to access.&lt;/p&gt;
&lt;p&gt;There is a larger section of memory (32MB) that is reserved from top of the SDRAM and shared with the Epiphany.
This is called 'external memory' from the perspective of the Epiphany.  It's also called 'shared memory'.
It is much slower to access from the Epiphany.&lt;/p&gt;
&lt;p&gt;The shared memory and memory locations inside the chip are both mapped in the address space of the host, and can be access by the host.
Locations in the chip include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;32K local to each core&lt;/li&gt;
&lt;li&gt;Registers on each core&lt;/li&gt;
&lt;li&gt;Chip-wide registers &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Something interesting here is that the registers all have memory mappings.
That means the registers can be accessed by the host (and other cores) by simply reading or writing a specific memory location.
(It is important to note that register values are only valid when the core is halted.)&lt;/p&gt;
&lt;h3&gt;Epiphany Software Development Kit&lt;/h3&gt;
&lt;p&gt;The ESDK contains some utilities to access these memory regions from the command line.
The commands &lt;code&gt;e-read&lt;/code&gt; and &lt;code&gt;e-write&lt;/code&gt; are used to read and write the locations.
To access the core-local memory, use row/column coordinate of the core (0-3 for each), followed by the offset.
For reading, optionally add the number of 4-byte words to read.  For writing, give a list of 4-byte word values.&lt;/p&gt;
&lt;p&gt;For example, to read 8 bytes from core (0,0)&lt;/p&gt;
&lt;pre class="code literal-block"&gt;parallella@parallella:~$ e-read 0 0 0x100 2
[0x00000100] = 0x782de028
[0x00000104] = 0x0d906c89
&lt;/pre&gt;


&lt;p&gt;To access the external memory, use a single -1 instead of the row,col coordinates.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;parallella@parallella:~$ e-write -1 0 1 2 3
[0x00000000] = 0x00000001
[0x00000004] = 0x00000002
[0x00000008] = 0x00000003
&lt;/pre&gt;


&lt;p&gt;Upon power up, it appears the memory is filled with random values.
The &lt;code&gt;epiphany-examples&lt;/code&gt; directory contains some useful utilities in the &lt;code&gt;apps&lt;/code&gt; directory.
To fill memory with some values, use &lt;code&gt;e-fill-mem&lt;/code&gt; (build it by running the &lt;code&gt;build.sh&lt;/code&gt; script first)&lt;/p&gt;
&lt;p&gt;To zero all the local memory in core 0, 0:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;parallella@parallella:~/epiphany-examples/apps/e-fill-mem$ ./bin/e-fill-mem.elf  0 0 1 1 8192 0
&lt;/pre&gt;


&lt;p&gt;Verify a few locations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;parallella@parallella:~/epiphany-examples/apps/e-fill-mem$ e-read 0 0 0 4
[0x00000000] = 0x00000000
[0x00000004] = 0x00000000
[0x00000008] = 0x00000000
[0x0000000c] = 0x00000000
&lt;/pre&gt;


&lt;p&gt;Nostalgia sidebar: If you want to reminisce about the days of Commodore 64, Apple II's and other microcomputers, alias &lt;code&gt;e-read&lt;/code&gt; and &lt;code&gt;e-write&lt;/code&gt; to &lt;code&gt;peek&lt;/code&gt; and &lt;code&gt;poke&lt;/code&gt;. (For the bash shell that would be &lt;code&gt;alias peek=e-read&lt;/code&gt; and &lt;code&gt;alias poke=e-write&lt;/code&gt;)&lt;/p&gt;
&lt;h3&gt;Simple example of local memory access&lt;/h3&gt;
&lt;p&gt;To solidify understanding of how this works, let's try a simple program that adds two numbers in core-local memory,
and saves the result to another location in core-local memory.  We will use the command line tools to set memory and verify
the operation. &lt;/p&gt;
&lt;p&gt;The 32KB of local memory puts all the offsets in the range 0x0000 - 0x8000.   Let's choose a base location 0x2000, which will be above the executable code, and below the stack.&lt;/p&gt;
&lt;p&gt;Start with the following C program (mem.c)&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;// Demonstrate local memory access at a low level&lt;/span&gt;

&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;// Location in local memory will not interfere&lt;/span&gt;
    &lt;span class="c1"&gt;// with the executable or the stack&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;outbuf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="mh"&gt;0x2000&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outbuf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outbuf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
    &lt;span class="n"&gt;outbuf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Compile with &lt;/p&gt;
&lt;pre class="code literal-block"&gt;e-gcc mem.c -T /opt/adapteva/esdk/bsps/current/fast.ldf -o mem.elf
&lt;/pre&gt;


&lt;p&gt;The -T option refers to a linker script, which controls where various pieces of the executable are placed in memory.&lt;/p&gt;
&lt;p&gt;Set the initial memory locations&lt;/p&gt;
&lt;pre class="code literal-block"&gt;parallella@parallella:~$ e-write 0 0 0x2000 1 2 0
[0x00002000] = 0x00000001
[0x00002004] = 0x00000002
[0x00002008] = 0x00000000
&lt;/pre&gt;


&lt;p&gt;Load and run the program (the -s option to e-loader runs the program after loading)&lt;/p&gt;
&lt;pre class="code literal-block"&gt;parallella@parallella:~$ e-loader -s mem.elf 
Loading program "mem.elf" on cores (0,0)-(0,0)
e_set_loader_verbosity(): setting loader verbosity to 1.
e_load_group(): loading ELF file mem.elf ...
e_load_group(): send SYNC signal to core (0,0)...
e_start(): SYNC (0xf042c) to core (0,0)...
e_start(): done.
e_load_group(): done.
e_load_group(): done loading.
e_load_group(): closed connection.
e_load_group(): leaving loader.
&lt;/pre&gt;


&lt;p&gt;Now verify the program produced the expected result:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;parallella@parallella:~$ e-read 0 0 0x2000 3
[0x00002000] = 0x00000001
[0x00002004] = 0x00000002
[0x00002008] = 0x00000003
&lt;/pre&gt;


&lt;p&gt;Yes.  It worked!&lt;/p&gt;
&lt;p&gt;Now we've seen some concrete low-level details on how memory works on the Parallella and Epiphany.
Next time I want to look the e-loader in more detail, and how programs start running on the cores.&lt;/p&gt;&lt;/div&gt;</description><category>epiphany</category><category>parallella</category><guid>https://markdewing.github.io/blog/posts/communicating-with-epiphany/</guid><pubDate>Tue, 25 Aug 2015 19:19:00 GMT</pubDate></item><item><title>Introduction to Parallella</title><link>https://markdewing.github.io/blog/posts/introduction-to-parallella/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;The Parallella is a single board computer with a dual core ARM and a 16 core Epiphany coprocessor. 
I've had some boards sitting around after backing the Kickstarter, and now I've finally started to play with them.&lt;/p&gt;
&lt;p&gt;The main purpose of the board is to highlight the Ephiphany coprocessor, but it has other interesting
resources as well.  I'd like to look into how to use each of them.&lt;/p&gt;
&lt;p&gt;Resources to program:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Xilinx Zynq (7010 or 7020), which contains&lt;ul&gt;
&lt;li&gt;dual core ARM Cortex A9 processors (with NEON SIMD instructions)&lt;/li&gt;
&lt;li&gt;FPGA&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Epiphany 16 core coprocessor (simple cores in a grid)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See the website (&lt;a href="http://parallella.org"&gt;parallella.org&lt;/a&gt;) for more &lt;a href="http://parallella.org/board"&gt;details of the board&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After getting the system set up and running according to the &lt;a href="https://www.parallella.org/quick-start/"&gt;directions&lt;/a&gt;, the first question is how 
to compile code?   Since there are two architectures on the board, it gets a bit more complex.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regular PC (in my case, 64 bit, running Ubuntu) - the host for cross compilation, targeting either the ARM cores or the Epiphany.&lt;/li&gt;
&lt;li&gt;ARM on Zynq - can be a cross-compilation target, can compile for itself, or can compile for the Epiphany&lt;/li&gt;
&lt;li&gt;Epiphany - only a target&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While code can be compiled on the board, compiling on host PC can have some definite advantages with much larger resources of disk space, disk speed, etc.
However, setting up projects for cross-compiliation can be more challenging.&lt;/p&gt;
&lt;h2&gt;Cross compiling to ARM&lt;/h2&gt;
&lt;p&gt;On Ubuntu, this turns out to be fairly easy - the compiler packages that target ARM are already available in the repository.&lt;/p&gt;
&lt;p&gt;Using the Ubuntu Software Center (or Synaptic, or the apt- tools, as you prefer), install the following packages&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;gcc-arm-linux-gnueabihf&lt;/li&gt;
&lt;li&gt;binutils-arm-linux-gnueabihf&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Selecting these should install the necessary dependencies (some listed here):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;libc6-armhf-cross&lt;/li&gt;
&lt;li&gt;libc6-dev-armhf-cross&lt;/li&gt;
&lt;li&gt;cpp-arm-linux-gnueabihf&lt;/li&gt;
&lt;li&gt;gcc-4.8-multilib-arm-linux-gnueabihf&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(By the way, the 'hf' at the end stands for 'Hard Float' - it means the processor has floating point in hardware)&lt;/p&gt;
&lt;p&gt;See this &lt;a href="https://parallella.org/forums/viewtopic.php?f=13&amp;amp;t=935"&gt;forum post&lt;/a&gt; for more information.  That post also contains instructions for setting up Eclipse (I'm more partial to the command line).&lt;/p&gt;
&lt;p&gt;To cross compile using the command line, all the normal compiler tools are prefixed with &lt;code&gt;arm-linux-gnueabihf&lt;/code&gt;.  Use &lt;code&gt;arm-linux-gnueabihf-gcc -o hello hello.c&lt;/code&gt; to compile a simple example.&lt;/p&gt;
&lt;p&gt;Run &lt;code&gt;file&lt;/code&gt; on the output file to verify it compiled as an ARM executable.&lt;/p&gt;
&lt;h3&gt;Clang&lt;/h3&gt;
&lt;p&gt;Compiling with clang needs at least the include and lib files from the 'libc6-*-armhf-cross' packages.&lt;/p&gt;
&lt;p&gt;Assuming the version of clang is built to output the 'arm' target, the following should work&lt;/p&gt;
&lt;pre class="code literal-block"&gt;clang -target arm-linux-guneabihf -I /usr/arm-linux-gnueabihf/include hello.cpp
&lt;/pre&gt;


&lt;h2&gt;Cross compiling to Epiphany&lt;/h2&gt;
&lt;p&gt;These are the tools in the ESDK.&lt;/p&gt;
&lt;p&gt;If using the ARM as a host, the ESDK is already in the microSD images and the tools are in the path (&lt;code&gt;\opt\adapteva\esdk\tools\e-gnu\bin&lt;/code&gt;)
The tools are prefixed with &lt;code&gt;e-&lt;/code&gt;.  Use &lt;code&gt;e-gcc&lt;/code&gt; to invoke the compiler.&lt;/p&gt;
&lt;p&gt;For a Linux host, download and install the ESDK from the website (under &lt;code&gt;Software -&amp;gt; Pre-built -&amp;gt; Epiphany SDK&lt;/code&gt;)(&lt;a href="ftp://ftp.parallella.org/esdk"&gt;direct link&lt;/a&gt;).  Look for 'linux_x86_64' in the file name.&lt;/p&gt;
&lt;p&gt;The ESDK has examples you can compile and run.  Sometime later I want to take a closer look at how the Epiphany files are loaded to the coprocessor and run.&lt;/p&gt;&lt;/div&gt;</description><category>epiphany</category><category>parallella</category><category>zynq</category><guid>https://markdewing.github.io/blog/posts/introduction-to-parallella/</guid><pubDate>Thu, 20 Aug 2015 20:08:00 GMT</pubDate></item><item><title>Introduction</title><link>https://markdewing.github.io/blog/posts/introduction/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;p&gt;This blog will cover whatever bits and pieces of computer technology I find interesting.
Science and research topics can be found over at my &lt;a href="http://quantum_mc.blogspot.com/"&gt;Quantum Monte Carlo&lt;/a&gt; blog.&lt;/p&gt;</description><guid>https://markdewing.github.io/blog/posts/introduction/</guid><pubDate>Thu, 20 Aug 2015 04:18:00 GMT</pubDate></item></channel></rss>