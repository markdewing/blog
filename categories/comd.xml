<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Journey to the Center of the Computer (CoMD)</title><link>https://markdewing.github.io/blog/</link><description></description><atom:link href="https://markdewing.github.io/blog/categories/comd.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Fri, 13 Nov 2015 23:44:46 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>More Performance With Numba</title><link>https://markdewing.github.io/blog/posts/more-with-numba/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;Having acquired a shiny new &lt;a href="http://markdewing.github.io/blog/posts/prototype-for-profiling-python/"&gt;profiler&lt;/a&gt;, it's time to dig into the performance of the Numba version some more.&lt;/p&gt;
&lt;p&gt;Picking up from the &lt;a href="http://markdewing.github.io/blog/posts/improvements-in-comd-cell-method-performance/"&gt;previous optimizations&lt;/a&gt;, I can't seem to reproduce the timing (47 μs/atom) in the that table.  Now I get 40 μs/atom.&lt;/p&gt;
&lt;h2&gt;First step&lt;/h2&gt;
&lt;p&gt;Run the profiler (&lt;code&gt;vmprofrun comd.py&lt;/code&gt;) and display the results in KCacheGrind (&lt;code&gt;kcachegrind vmprof-20664.out&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;Sorting by self time, we see &lt;code&gt;getBoxFromCoord&lt;/code&gt; at the top:&lt;/p&gt;
&lt;p&gt;&lt;img alt="KCachegrind screenshot of functions sorted by self time" src="https://markdewing.github.io/blog/2015/profile1_by_self_sm.png"&gt;&lt;/p&gt;
&lt;p&gt;Also a screen shot of the call graph - &lt;code&gt;getBoxFromCoord&lt;/code&gt; gets called from two different places - &lt;code&gt;putAtomInBox&lt;/code&gt; and &lt;code&gt;updateLinkCells&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="KCachegrind screenshot of call graph" src="https://markdewing.github.io/blog/2015/profile1_call_graph_sm.png"&gt;&lt;/p&gt;
&lt;p&gt;To improve performance here, convert &lt;code&gt;getBoxFromCoord&lt;/code&gt; to a free function and put all the attribute references into function arguments.&lt;/p&gt;
&lt;p&gt;Before:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;getBoxFromCoord&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;floor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;invBoxSize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="n"&gt;iy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;floor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;invBoxSize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="n"&gt;iz&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;floor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;invBoxSize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getBoxFromTuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;iy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;iz&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;After:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="nd"&gt;@numba.njit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;getBoxFromCoordInner&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;invBoxSize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nLocalBoxes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;floor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;invBoxSize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
    &lt;span class="n"&gt;iy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;floor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;invBoxSize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
    &lt;span class="n"&gt;iz&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;floor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;invBoxSize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;getBoxFromTupleInner&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;iy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;iz&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nLocalBoxes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;(Changing the parameter &lt;code&gt;r&lt;/code&gt; to individual components was not strictly necessary.)&lt;/p&gt;
&lt;p&gt;And the call sites change (for example) from&lt;/p&gt;
&lt;pre class="code literal-block"&gt;        &lt;span class="n"&gt;iBox&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getBoxFromCoord&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;to&lt;/p&gt;
&lt;pre class="code literal-block"&gt;        &lt;span class="n"&gt;iBox&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;getBoxFromCoordInner&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;invBoxSize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nLocalBoxes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gridSize&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;This improves performance to 20 μs/atom.&lt;/p&gt;
&lt;p&gt;Repeating the same transformation for putAtomInBox gives 18.4 μs/atom.&lt;/p&gt;
&lt;h2&gt;Second step&lt;/h2&gt;
&lt;p&gt;Run the profiler again.  By self time, &lt;code&gt;loadAtomsBuffer&lt;/code&gt; is at the top.  Let's look at that in context.
Sort by inclusive time, and we see that the parts of the call tree starting at &lt;code&gt;redistributeAtoms&lt;/code&gt; take a significant amount of time.&lt;/p&gt;
&lt;p&gt;&lt;img alt="KCachegrind screenshot of functions sorted by inclusive time" src="https://markdewing.github.io/blog/2015/profile2_by_incl_sm.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="KCachegrind screenshot of call graph" src="https://markdewing.github.io/blog/2015/profile2_call_graph_sm.png"&gt;&lt;/p&gt;
&lt;p&gt;This part of the code:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Applies periodic boundary conditions&lt;/li&gt;
&lt;li&gt;Moves atoms to a new cell&lt;/li&gt;
&lt;li&gt;Packs atom into a buffer at source&lt;/li&gt;
&lt;li&gt;Unpacks buffer into atom data structure at destination&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This packing/unpacking anticipates the parallel version, which transmits the buffer across processors.&lt;/p&gt;
&lt;p&gt;A previous attempt at using numpy records did not work well (and ran into a serious performance regression with numpy 1.10).
This time I went with two buffers - one for integers, and one for floating point numbers.  This works better, and the
performance is now 10.2 μs/atom.&lt;/p&gt;
&lt;h2&gt;More steps&lt;/h2&gt;
&lt;p&gt;Continuing the process of profiling, and converting routines to be Numba friendly eventually reached a performance of 2.9 μs/atom.
(Wow, this is only about 30% slower than C.)&lt;/p&gt;
&lt;p&gt;Modified code is &lt;a href="https://gist.github.com/markdewing/8bd6bd8dbef8613004fe"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The updated performance table is&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language/compiler  &lt;/th&gt;
&lt;th&gt;Version   &lt;/th&gt;
&lt;th align="right"&gt;Initial time&lt;/th&gt;
&lt;th align="right"&gt;  Final time&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Python&lt;/td&gt;
&lt;td&gt;2.7.10&lt;/td&gt;
&lt;td align="right"&gt;1014&lt;/td&gt;
&lt;td align="right"&gt;1014&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PyPy&lt;/td&gt;
&lt;td&gt;4.0&lt;/td&gt;
&lt;td align="right"&gt;30&lt;/td&gt;
&lt;td align="right"&gt;30&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cython&lt;/td&gt;
&lt;td&gt;0.23.3&lt;/td&gt;
&lt;td align="right"&gt;729&lt;/td&gt;
&lt;td align="right"&gt;13&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julia&lt;/td&gt;
&lt;td&gt;0.4.0-rc3&lt;/td&gt;
&lt;td align="right"&gt;87&lt;/td&gt;
&lt;td align="right"&gt;6.1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Numba&lt;/td&gt;
&lt;td&gt;0.22.1&lt;/td&gt;
&lt;td align="right"&gt;867&lt;/td&gt;
&lt;td align="right"&gt;2.9&lt;/td&gt;
&lt;td&gt;    New result&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;C (clang)&lt;/td&gt;
&lt;td&gt;3.7&lt;/td&gt;
&lt;td align="right"&gt;2.2&lt;/td&gt;
&lt;td align="right"&gt;2.2&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;
&lt;/p&gt;&lt;div style="font-size:80%"&gt;
Times are all in μs/atom. System size is 4000 atoms.
Hardware is Xeon E5-2630 v3 @ 2.4 Ghz, OS is Ubuntu 12.04.
&lt;br&gt;
The 'Initial Time' column results from the minimal amount of code changes to get the compiler working.
&lt;br&gt;
The 'Final Time' is the time after tuning.
&lt;/div&gt;
&lt;br&gt;
&lt;p&gt;Do note that the Cython, Julia, and Numba results reflect the amount of effort put into optimization.
Cython and Julia still need to be improved with the assistance of a profiler (or in Julia's case, a better viewer
for existing profile data).&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Using a profiler to guide our optimization efforts has been very helpful.&lt;/p&gt;
&lt;p&gt;The Numba results are really promising, but, in addition to creating ugly code, it required an amount of work
that I would not want to perform on a regular basis.    These transformations are fairly regular, so it should
be possible to incorporate them into Numba.  Alternately, if doing so in a safe manner inside the compiler is difficult,
some sort of automated AST transformation of the source should be possible.&lt;/p&gt;
&lt;p&gt;As the optimization process proceeds on this code, increasing amount of time is being spent in the core routine, computeForce, (as it should), and we will need to move beyond a function-level profiler to look for optimization opportunities.&lt;/p&gt;&lt;/div&gt;</description><category>CoMD</category><category>Numba</category><category>python</category><guid>https://markdewing.github.io/blog/posts/more-with-numba/</guid><pubDate>Fri, 13 Nov 2015 16:30:00 GMT</pubDate></item><item><title>Performance Updates with PyPy 4.0</title><link>https://markdewing.github.io/blog/posts/updated-performance-with-pypy-40/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;The PyPy team recently released version &lt;a href="http://morepypy.blogspot.com/2015/10/pypy-400-released-jit-with-simd.html"&gt;4.0&lt;/a&gt;
(The jump in version number is to reduce confusion with the Python version supported.)
One of the features is improved performance.&lt;/p&gt;
&lt;p&gt;But first, an issue with reporting accurate timings with this version of CoMD should be addressed. The initial iteration contains overhead from tracing and JIT compilation (Cython and Numba have the same issue).
For this example we are concerned with the steady-state timing, so the time for the first iteration should be excluded.
I've added a '--skip' parameter to the CoMD code (default: 1) that skips the first &lt;code&gt;printRate&lt;/code&gt; steps (default: 10) in computing the overall average update rate at the end.&lt;/p&gt;
&lt;p&gt;Now the table with the most recent performance numbers  (from &lt;a href="http://markdewing.github.io/blog/posts/improvements-in-comd-cell-method-performance/"&gt;this post&lt;/a&gt; ), updated with PyPy 4.0 results:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language/compiler  &lt;/th&gt;
&lt;th&gt;Version   &lt;/th&gt;
&lt;th align="right"&gt;Time&lt;/th&gt;
&lt;th align="right"&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Python&lt;/td&gt;
&lt;td&gt;2.7.10&lt;/td&gt;
&lt;td align="right"&gt;1014&lt;/td&gt;
&lt;td align="right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PyPy&lt;/td&gt;
&lt;td&gt;2.6.1&lt;/td&gt;
&lt;td align="right"&gt;96&lt;/td&gt;
&lt;td align="right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Numba&lt;/td&gt;
&lt;td&gt;0.21.0&lt;/td&gt;
&lt;td align="right"&gt;47&lt;/td&gt;
&lt;td align="right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PyPy&lt;/td&gt;
&lt;td&gt;4.0&lt;/td&gt;
&lt;td align="right"&gt;30&lt;/td&gt;
&lt;td align="right"&gt;    New result&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cython&lt;/td&gt;
&lt;td&gt;0.23.3&lt;/td&gt;
&lt;td align="right"&gt;13&lt;/td&gt;
&lt;td align="right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julia&lt;/td&gt;
&lt;td&gt;0.4.0-rc3&lt;/td&gt;
&lt;td align="right"&gt;6.1&lt;/td&gt;
&lt;td align="right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;C&lt;/td&gt;
&lt;td&gt;4.8.2&lt;/td&gt;
&lt;td align="right"&gt;2.3&lt;/td&gt;
&lt;td align="right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;
Times are all in μs/atom. System size is 4000 atoms.
Hardware is Xeon E5-2630 v3 @ 2.4 Ghz, OS is Ubuntu 12.04.
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The new release of PyPy also includes some SIMD vectorization support (&lt;code&gt;--jit vec=1&lt;/code&gt; or &lt;code&gt;--jit vec_all=1&lt;/code&gt;).  Neither of these provided any improvement in performance on this code.   Not too surprising given the vectorization support is new, and the code contains
conditionals in the inner loop.&lt;/p&gt;
&lt;p&gt;PyPy 4.0 gives a very good 34x performance improvement over bare Python, and 3x improvement over the previous release (2.6.1).
PyPy is attractive here in that no modifications made to the source.  (Both Cython and Numba required source changes)&lt;/p&gt;&lt;/div&gt;</description><category>CoMD</category><category>PyPy</category><guid>https://markdewing.github.io/blog/posts/updated-performance-with-pypy-40/</guid><pubDate>Wed, 04 Nov 2015 21:39:00 GMT</pubDate></item><item><title>Improvements in CoMD Cell Method Performance</title><link>https://markdewing.github.io/blog/posts/improvements-in-comd-cell-method-performance/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;A &lt;a href="http://markdewing.github.io/blog/posts/first-performance-improvements/"&gt;previous post&lt;/a&gt; showed some performance improvements with the &lt;em&gt;nsquared&lt;/em&gt; version of the code.
This post will tackle the &lt;em&gt;cell&lt;/em&gt; version of the code.
In the &lt;em&gt;nsquared&lt;/em&gt; version the time-consuming inner loop had no function calls.
The &lt;em&gt;cell&lt;/em&gt; version does call other functions, which may complicate optimization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language/compiler  &lt;/th&gt;
&lt;th&gt;Version   &lt;/th&gt;
&lt;th align="right"&gt;Initial time&lt;/th&gt;
&lt;th align="right"&gt;  Final time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;C&lt;/td&gt;
&lt;td&gt;4.8.2&lt;/td&gt;
&lt;td align="right"&gt;2.3&lt;/td&gt;
&lt;td align="right"&gt;2.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Python&lt;/td&gt;
&lt;td&gt;2.7.10&lt;/td&gt;
&lt;td align="right"&gt;1014&lt;/td&gt;
&lt;td align="right"&gt;1014&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PyPy&lt;/td&gt;
&lt;td&gt;2.6.1&lt;/td&gt;
&lt;td align="right"&gt;96&lt;/td&gt;
&lt;td align="right"&gt;96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julia&lt;/td&gt;
&lt;td&gt;0.4.0-rc3&lt;/td&gt;
&lt;td align="right"&gt;87&lt;/td&gt;
&lt;td align="right"&gt;6.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cython&lt;/td&gt;
&lt;td&gt;0.23.3&lt;/td&gt;
&lt;td align="right"&gt;729&lt;/td&gt;
&lt;td align="right"&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Numba&lt;/td&gt;
&lt;td&gt;0.21.0&lt;/td&gt;
&lt;td align="right"&gt;867&lt;/td&gt;
&lt;td align="right"&gt;47&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;
Times are all in μs/atom. System size is 4000 atoms.
Hardware is Xeon E5-2630 v3 @ 2.4 Ghz, OS is Ubuntu 12.04.
&lt;br&gt;
The 'Initial Time' column results from the minimal amount of code changes to get the compiler working.
&lt;br&gt;
The 'Final Time' is the time after the tuning in this post.&lt;/p&gt;
&lt;h3&gt;Julia&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;cell&lt;/em&gt; version contains the same issue with array operations as the &lt;em&gt;nsquared&lt;/em&gt; version - the computation of &lt;code&gt;dd&lt;/code&gt; allocates a temporary to hold the results every time through the loop.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/lindahua/Devectorize.jl"&gt;Devectorize&lt;/a&gt; package can automatically convert array notation
to a loop.  If we add the &lt;code&gt;@devec&lt;/code&gt; annotation, the time drops to 43 μs/atom.
Unfortunately, the allocation to hold the result must still be performed, and it remains inside the inner particle loop.
If we manually create the loop and hoist the allocation out of the loop, time is 27 μs/atom.&lt;/p&gt;
&lt;p&gt;The code uses &lt;code&gt;dot&lt;/code&gt; to compute the vector norm.  This calls a routine (&lt;code&gt;julia_dot&lt;/code&gt;) to perform the
dot product.
For long vectors calling an optimized linear algebra routine is beneficial, but for a vector of length 3 this adds overhead.
Replacing &lt;code&gt;dot&lt;/code&gt; with the equivalent loop reduces the time to 23 μs/atom.&lt;/p&gt;
&lt;p&gt;Looking through the memory allocation output (&lt;code&gt;--track-allocation=user&lt;/code&gt;) shows some vector operations
when the force array is zeroed and accumulated.
Also in &lt;code&gt;putAtomInBox&lt;/code&gt; in &lt;code&gt;linkcell.jl&lt;/code&gt;.
 These spots are also visible in the profile output, but the profile output is less convenient because it is not shown with source.
The &lt;code&gt;@devec&lt;/code&gt; macro does work here, and the performance is now 7.7 μs/atom.   Explicit loops
give a slightly better time of 7.3 μs/atom.&lt;/p&gt;
&lt;p&gt;Profiling shows even more opportunities for devectorization in &lt;code&gt;advanceVelocity&lt;/code&gt; and &lt;code&gt;advancePosition&lt;/code&gt; in &lt;code&gt;simflat.jl&lt;/code&gt;  Time is now 6.4 μs/atom.&lt;/p&gt;
&lt;p&gt;The Julia executable has a &lt;code&gt;-O&lt;/code&gt; switch for more time-intensive optimizations (it adds more LLVM optimization passes).   This improves the time to 6.2 μs/atom.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;@fastmath&lt;/code&gt; macro improves the time a little more, to 6.1 μs/atom.
The &lt;code&gt;@inbounds&lt;/code&gt; macro to skip the bounds checks did not seem to improve the time.&lt;/p&gt;
&lt;p&gt;The final Julia time is now within a factor of 3 of the C time.  The code is &lt;a href="https://gist.github.com/markdewing/54709a0fd6a17348a7cb"&gt;here&lt;/a&gt;.  It's not clear where the remaining time overhead comes from. &lt;/p&gt;
&lt;h3&gt;PyPy&lt;/h3&gt;
&lt;p&gt;The PyPy approach to JIT compilation is very general, but that also makes it difficult to target what code
changes might improve performance.
The &lt;a href="https://bitbucket.org/pypy/jitviewer"&gt;Jitviewer&lt;/a&gt; tool is nice, but not helpful at a cursory glance.
The &lt;a href="https://vmprof.readthedocs.org/en/latest/"&gt;vmprof&lt;/a&gt; profiler solves an important problem by collecting the native code stack plus the python stack. 
In this particular case, it reports at the function level, and the bulk of the time was spent in &lt;code&gt;computeForce&lt;/code&gt;.
I hope to write more about vmprof in a future post, as it could help with integrated profiling of Python + native code (either compiled or JIT-ed).&lt;/p&gt;
&lt;h3&gt;Cython&lt;/h3&gt;
&lt;p&gt;The simplest step is to add an initialization line and move some &lt;code&gt;.py&lt;/code&gt; files to &lt;code&gt;.pyx&lt;/code&gt; files.  This gives 729 μs/atom.
Adding types to the computeForce function and assigning a few attribute lookups to local variables so the types can be assigned (playing a game of 'remove the yellow' in the Cython annotation output) gives 30 μs/atom.&lt;/p&gt;
&lt;p&gt;Adding types and removing bounds checks more routines  (in  &lt;code&gt;halo.py&lt;/code&gt;, &lt;code&gt;linkcell.py&lt;/code&gt;, &lt;code&gt;simflat.py&lt;/code&gt;) gives 13 μs/atom.&lt;/p&gt;
&lt;p&gt;Code is &lt;a href="https://gist.github.com/markdewing/3688c6eebc0a88081e07"&gt;here&lt;/a&gt;.
Further progress needs deeper investigation with profiling tools.&lt;/p&gt;
&lt;h3&gt;Numba&lt;/h3&gt;
&lt;p&gt;Starting with adding &lt;code&gt;@numba.jit&lt;/code&gt; decorators to &lt;code&gt;computeForce&lt;/code&gt;, and the functions it calls gives the
initial time of 867 μs/atom.
Extracting all the attribute lookups (including the function call to &lt;code&gt;getNeighborBoxes&lt;/code&gt;) gives 722 μs/atom.&lt;/p&gt;
&lt;p&gt;We should ensure the call to &lt;code&gt;getNeighborBoxes&lt;/code&gt; is properly JIT-ed.  Unfortunately, this requires more involved
code restructuring.  Functions need to be split into a wrapper that performs any needed attribute lookups, and
an inner function that gets JIT-ed.  Loop lifting automatically performs this transformation on functions
  with loops.  On functions without loops, however, it needs to be done manually.
Once this is done, the time improves dramatically to 47 μs/atom.&lt;/p&gt;
&lt;p&gt;Hopefully the upcoming "JIT Classes" feature will make this easier, and require less code restructuring. &lt;/p&gt;
&lt;p&gt;Code is &lt;a href="https://gist.github.com/markdewing/89cce577f5b8625cc776"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;Julia is leading in terms of getting the best performance on this example.  Many of these projects are rapidly improving, so this is just a snapshot at their current state.&lt;/p&gt;
&lt;p&gt;All these projects need better profiling tools to show the user where code is slow and to give feedback on why the code is slow.
The Cython annotated output is probably the the best - it highlights which lines need attention. 
However it is not integrated with profiler output, so in a project of any size, it's not clear where a user should spend time adding types.  &lt;/p&gt;
&lt;p&gt;Julia has some useful collection and feedback tools, but they would be much more helpful if combined.  The memory allocation output is bytes allocated.
It's useful for finding allocations where none were expected, or for allocations in known hot loops, but it's less clear which other allocations are impacting performance.
Ideally this could be integrated with profiler output and weighted by time spent to show which allocations are actually affecting execution time.&lt;/p&gt;&lt;/div&gt;</description><category>CoMD</category><category>cython</category><category>julia</category><category>Numba</category><category>PyPy</category><category>python</category><guid>https://markdewing.github.io/blog/posts/improvements-in-comd-cell-method-performance/</guid><pubDate>Fri, 02 Oct 2015 18:56:00 GMT</pubDate></item><item><title>Comparing languages with miniapps</title><link>https://markdewing.github.io/blog/posts/comparing-languages-with-miniapps/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;The &lt;a href="https://mantevo.org/"&gt;Mantevo project&lt;/a&gt; provides a collection of miniapps, which are simplified versions
of real scientific applications that make it easier to explore performance, scaling, languages, programming models, etc.
I want to focus on the language aspect and port some apps to new languages to see how they compare. &lt;/p&gt;
&lt;p&gt;The first miniapp I started with is &lt;a href="http://www.exmatex.org/comd.html"&gt;CoMD&lt;/a&gt;, a molecular dynamics code in C.&lt;/p&gt;
&lt;p&gt;For the language ports, I made multiple variants of increasing complexity.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;nsquared&lt;/dt&gt;
&lt;dd&gt;This uses the naive algorithm in computing inter-particle interactions. The central loop computes the
   interaction of every particle with every other particle.  The scaling of run time vs number of particles is N&lt;sup&gt;2&lt;/sup&gt;.&lt;/dd&gt;
&lt;dt&gt;cell&lt;/dt&gt;
&lt;dd&gt;The cell list method divides space into cells and tracks the particles in each cell.  When computing interactions, only the particles in neighboring cells need to be considered.  The scaling of run time vs. the number of particles is N.&lt;/dd&gt;
&lt;dt&gt;mpi&lt;/dt&gt;
&lt;dd&gt;Parallel version of the cell method.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;The C version corresponds to the 'cell' and 'mpi' variants (plus the C version has OpenMP and several other programming model variants)&lt;/p&gt;
&lt;p&gt;Currently there are Python and Julia ports for the nsquared and cell variants, and a Python version of the mpi variant.
They are available in my 'multitevo' github repository: &lt;a href="https://github.com/markdewing/multitevo"&gt;https://github.com/markdewing/multitevo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Julia version is a pretty straightforward port of the Python version, so it is probably not very idiomatic Julia code.
(I would be happy to take suggestions from the Julia community on how to improve the style and organization)&lt;/p&gt;
&lt;!--
C does not have multidimensional arrays so the 3D cell indices need to be mapped to a linear index of atom coordinates (and
the reverse), and this involves some complicated expressions.
The Julia and Python version preserve this approach, but since they have multidimensional arrays,
it might make the code much simpler to store the atom information directly in a multidimensional array.
--&gt;

&lt;h3&gt;Scaling with system size&lt;/h3&gt;
&lt;p&gt;First let's verify the scaling of the nsquared version vs. the cell list version (using the Julia versions).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Graph of scaling nsquared and cell list method" src="https://markdewing.github.io/blog/2015/scaling.png"&gt;&lt;/p&gt;
&lt;p&gt;As expected, the cell list variant has better scaling at larger system sizes.&lt;/p&gt;
&lt;!--
Data for nsquared variant
natoms time(s)   time_per_atom (us/atom)
256   0.011350  44.335089
500   0.034140  68.280823
864   0.099966 115.700927
1372   0.239239 174.372460
2048   0.509494 248.776288
2916   1.050071 360.106739
4000   1.942931 485.732711
5324   3.327456 624.991804
6912   5.769225 834.667932
--&gt;

&lt;!--
Data for cell method variant
natoms  time(s)   time_per_atom (us/atom)
256   0.045821 178.987427
500   0.049810  99.619442
864   0.142849 165.333940
1372   0.151120 110.145543
2048   0.310324 151.525439
2916   0.344807 118.246625
4000   0.344878  86.219497
5324   0.621829 116.797365
6912   0.626944  90.703648
--&gt;

&lt;h3&gt;Initial performance&lt;/h3&gt;
&lt;p&gt;For a purely computational code such as this, performance matters.
The ultimate goal is near C/Fortran speeds using a higher-level language to express the algorithm.&lt;/p&gt;
&lt;p&gt;Some initial timings (for a system size of 4000 atoms, using the cell variant)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language/Compiler  &lt;/th&gt;
&lt;th align="left"&gt;Version&lt;/th&gt;
&lt;th align="right"&gt;Time/atom (microseconds)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;C - gcc&lt;/td&gt;
&lt;td align="left"&gt;4.8.2&lt;/td&gt;
&lt;td align="right"&gt;2.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julia&lt;/td&gt;
&lt;td align="left"&gt;0.3.11&lt;/td&gt;
&lt;td align="right"&gt;153.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julia&lt;/td&gt;
&lt;td align="left"&gt;0.4.0-dev+6990&lt;/td&gt;
&lt;td align="right"&gt;88.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Python&lt;/td&gt;
&lt;td align="left"&gt;2.7.10 (from Anaconda)  &lt;/td&gt;
&lt;td align="right"&gt;941.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Numba&lt;/td&gt;
&lt;td align="left"&gt;0.20.0&lt;/td&gt;
&lt;td align="right"&gt;789.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pypy&lt;/td&gt;
&lt;td align="left"&gt;2.6.1&lt;/td&gt;
&lt;td align="right"&gt;98.4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;(Hardware is Xeon E5-2630 v3 @ 2.4 Ghz, OS is Ubuntu 12.04)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: These numbers indicate how a particular version of the language and compiler perform on a particular version of this code. The main purpose for these numbers is a baseline to measure future performance improvements.&lt;/p&gt;
&lt;p&gt;I tried &lt;a href="http://www.cosmology.ethz.ch/research/software-lab/HOPE.html"&gt;HOPE&lt;/a&gt;, a Python to C++ JIT compiler.
It require some modifications to the python code, but then failed in compiling the resulting C++ code.
I also tried &lt;a href="http://www.parakeetpython.com/"&gt;Parakeet&lt;/a&gt;.  It failed to translate the Python code, and I did not investigate further.&lt;/p&gt;
&lt;p&gt;It is clear when comparing to C there is quite a bit of room for improvement in the code using the high-level language compilers (Julia, Numba, PyPy).
Whether that needs to come from the modifications to the code, or improvements in the compilers, I don't know yet.&lt;/p&gt;
&lt;p&gt;The only real performance optimization so far has been adding type declarations to the composite types in Julia.
This boosted performance by about 3x. Without the type declarations, the Julia 0.4.0 speed is about 275 us/atom.
First performance lesson: Add type declarations to composite types in Julia.&lt;/p&gt;
&lt;p&gt;Julia and Numba have a number of similarities and so I want to focus on improving the performance of the code
under these two systems in the next few posts.&lt;/p&gt;
&lt;!--
Julia and Numba have similar stages (infer types, convert to LLVM IR, convert to native code), and so I hope to look
at them in parallel going forward.  
--&gt;&lt;/div&gt;</description><category>CoMD</category><category>julia</category><category>Mantevo</category><category>Numba</category><category>PyPy</category><category>python</category><guid>https://markdewing.github.io/blog/posts/comparing-languages-with-miniapps/</guid><pubDate>Wed, 02 Sep 2015 16:18:50 GMT</pubDate></item></channel></rss>