<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Journey to the Center of the Computer (Numba)</title><link>https://markdewing.github.io/blog/</link><description></description><atom:link href="https://markdewing.github.io/blog/categories/numba.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Fri, 18 Sep 2015 04:48:09 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>First Performance Improvements</title><link>https://markdewing.github.io/blog/posts/first-performance-improvements/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;The &lt;a href="http://markdewing.github.io/blog/posts/comparing-languages-with-miniapps/"&gt;previous post&lt;/a&gt; introduced the CoMD miniapp in Python and Julia.
This post will look a little deeper into the performance of Julia and various Python compilers.
I will use the nsquared version of the code for simplicity.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;computeForce&lt;/code&gt; function in &lt;code&gt;ljforce&lt;/code&gt; takes almost all the time, and it is here we should focus.
In the nsquared version there are no further function calls inside this loop, which should make it
easier to analyze and improve the performance.&lt;/p&gt;
&lt;p&gt;The summary performance table (all times are in microseconds/atom, the system is the smallest size at 256 atoms)&lt;/p&gt;
&lt;!-- from laptop --&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language/compiler  &lt;/th&gt;
&lt;th&gt;version   &lt;/th&gt;
&lt;th align="right"&gt;Initial time&lt;/th&gt;
&lt;th align="right"&gt;  Final time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Python&lt;/td&gt;
&lt;td&gt;2.7.10&lt;/td&gt;
&lt;td align="right"&gt;560&lt;/td&gt;
&lt;td align="right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PyPy&lt;/td&gt;
&lt;td&gt;2.6.1&lt;/td&gt;
&lt;td align="right"&gt;98&lt;/td&gt;
&lt;td align="right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HOPE&lt;/td&gt;
&lt;td&gt;0.4.0&lt;/td&gt;
&lt;td align="right"&gt;8&lt;/td&gt;
&lt;td align="right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cython&lt;/td&gt;
&lt;td&gt;0.23.1&lt;/td&gt;
&lt;td align="right"&gt;335&lt;/td&gt;
&lt;td align="right"&gt;8.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Numba&lt;/td&gt;
&lt;td&gt;0.21.0&lt;/td&gt;
&lt;td align="right"&gt;450&lt;/td&gt;
&lt;td align="right"&gt;8.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julia&lt;/td&gt;
&lt;td&gt;0.5.0-dev+50&lt;/td&gt;
&lt;td align="right"&gt;44&lt;/td&gt;
&lt;td align="right"&gt;7.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- From desktop 
| Python            | 2.7.10  | 438          |            |
| PyPy              |         |  84          |            |
| HOPE              |         |   7          |            |
| Cython            |         | 329          |  49        |
| Numba             |         | 405          |   7        |
| Julia             |         |  45          |  19        |
--&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;(This table uses a different code version and different hardware, so the numbers are not comparable to the previous post)
&lt;br&gt;
(Hardware is i7-3720QM @ 2.6 Ghz and OS is Ubuntu 14.04)
&lt;br&gt;
The 'Initial Time' column results from the minimal amount of code changes to get the compiler working.
&lt;br&gt;
The 'Final Time' is the time after doing some tuning (for this post, anyway - there's still more work to be done).&lt;/p&gt;
&lt;!--The level of tuning in this post is about adding type information and removing temporary memory allocations.
Further investigation of the generated intermediate and assembly code is yet to be done.
--&gt;

&lt;h3&gt;HOPE&lt;/h3&gt;
&lt;p&gt;The HOPE compiler will compile the nsquared version, provided the branch involving the cutoff is modified to avoid the &lt;code&gt;continue&lt;/code&gt; statement.
The loop that zeros the force also needs a simple name change to the loop variable.
The backend C++ compiler is gcc 4.8.4.  No attempt was made to try different compilers or further optimize the generated code.&lt;/p&gt;
&lt;h3&gt;Numba&lt;/h3&gt;
&lt;p&gt;To use Numba, we need to add &lt;code&gt;import numba&lt;/code&gt; to &lt;code&gt;ljforce.py&lt;/code&gt; and add the &lt;code&gt;@numba.jit&lt;/code&gt; decorator to the &lt;code&gt;computeForce&lt;/code&gt; method.
This gives the time in the initial column (450 μs/atom) , which is about a 20% improvement over the plain Python version.&lt;/p&gt;
&lt;p&gt;Code involving attribute lookups cannot currently be compiled to efficient code, and these lookups occur inside this inner loop.
And the compiler will not hoist attribute lookups outside the loop.
This can be done manually by assigning the attribute to a temporary variable before the loop, and replacing the values in the loop body. 
This transformation enables effective compilation of the loop.&lt;/p&gt;
&lt;p&gt;(Internally Numba performs loop-lifting, where it extracts the loop to a separate function in order to compile the loop.)&lt;/p&gt;
&lt;!--Loop-lifting is a way for Numba to extract a loop into a separate function in order to compile the loop.--&gt;

&lt;p&gt;The beginning of the function now looks like&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    &lt;span class="nd"&gt;@numba.jit&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;computeForce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sim&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c"&gt;# hoist the attribute lookups of these values&lt;/span&gt;
        &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;
        &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;
        &lt;span class="n"&gt;bounds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bounds&lt;/span&gt;
        &lt;span class="n"&gt;nAtoms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nAtoms&lt;/span&gt;
        &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;

        &lt;span class="c"&gt;# rest of original function&lt;/span&gt;
        &lt;span class="c"&gt;# ...&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Now Numba can compile the time consuming loop, and this gives about 9 μs/atom.&lt;/p&gt;
&lt;p&gt;The loop that zeros the force can be slightly improved by either looping over the last dimension
explicitly, or by zeroing the entire array at once.  This change yields the final number in the table (8.3 μs/atom).&lt;/p&gt;
&lt;p&gt;The whole modified file is &lt;a href="https://gist.github.com/markdewing/eb0bf52ea1b71995150a"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Cython&lt;/h3&gt;
&lt;p&gt;The simplest change to enable Cython is to move &lt;code&gt;ljforce.py&lt;/code&gt; to &lt;code&gt;ljforce.pyx&lt;/code&gt;, and add &lt;code&gt;import pyximport; pyximport.install()&lt;/code&gt; to the beginning of &lt;code&gt;simflat.py&lt;/code&gt;.
This initial time (335 μs/atom) gives a 40% improvement over regular python, but there is more performance available.&lt;/p&gt;
&lt;p&gt;The first step is to add some type information.
In order to do this we need to hoist the attribute lookups and assign to temporary variables, as in the Numba version.
At this step, we add types for the same variables as the Numba version.
The beginning of the function looks like this:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;computeForce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="n"&gt;nAtoms&lt;/span&gt;
        &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="n"&gt;double&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;
        &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;
        &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;
        &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double_t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;bounds&lt;/span&gt;

        &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;
        &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;
        &lt;span class="n"&gt;bounds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bounds&lt;/span&gt;
        &lt;span class="n"&gt;nAtoms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nAtoms&lt;/span&gt;
        &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;

        &lt;span class="c"&gt;# rest of original function&lt;/span&gt;
        &lt;span class="c"&gt;# ...&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;The time for this change about 54 μs/atom.&lt;/p&gt;
&lt;p&gt;Cython has a convenient feature that creates an annotated HTML file highlighting lines in the 
original file that may causing a performance issue.  Run &lt;code&gt;cython -a ljforce.pyx&lt;/code&gt; to get the report.
This indicates some more type declarations need to be added.
Adding these types gives about 8.6 μs/atom.   Finally a decorator can be added to remove bounds checks (&lt;code&gt;@cython.boundscheck(False)&lt;/code&gt;) to get the final performance in the table (8.3 μs/atom).&lt;/p&gt;
&lt;p&gt;The whole &lt;code&gt;ljforce.pyx&lt;/code&gt; file is &lt;a href="https://gist.github.com/markdewing/7017e23c883a2bd297cb"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Julia&lt;/h3&gt;
&lt;p&gt;The biggest issue in this code seems to be allocations inside the inner loop.
The memory performance tools can indicate where unexpected allocations are occurring.
One tool is to use a command line option (&lt;code&gt;--track-allocation=user&lt;/code&gt;) to the julia executable.&lt;/p&gt;
&lt;p&gt;One problem is a temporary created inside the loop to hold the results of an array operation (the line that computes &lt;code&gt;dd&lt;/code&gt;).
Moving this allocation out of the loop and setting each element separately improves performance 
to 19 μs/atom.
Another allocation occurs when updating the force array using a slice.  Changing this to explicitly loop
over the elements improves the performance to the final numbers shown in the table (7.1 μs/atom).&lt;/p&gt;
&lt;p&gt;The final modified file is &lt;a href="https://gist.github.com/markdewing/f1c9a46a2fec8a3ade4f"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;The performance numbers have quite a bit of variance, and they are not a result of a rigorous benchmarking and statistics collection.  If you want to compare between compilers, the final results should probably be read something like: "The performance of Cython and Numba is roughly the same on this code, and Julia is a little bit faster for this code".
Also keep in mind we're not done yet digging into the performance of these different compilers.&lt;/p&gt;
&lt;p&gt;Some simple changes to the code can give dramatic performance improvements, but the difficulty is
discovering what changes need to be made and where to make them.&lt;/p&gt;
&lt;p&gt;Future topics to explore:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply these lessons to the cell version of the code.&lt;/li&gt;
&lt;li&gt;With Julia and Numba, it's hard to connect intermediate code stages (internal IR, LLVM IR, assembly) to the original code, and to spot potential performance issues there.  The Cython annotation output is nice here.&lt;/li&gt;
&lt;li&gt;The difference between operating on dynamic objects versus the underlying value types.&lt;/li&gt;
&lt;li&gt;How well does the final assembly utilize the hardware. How to use hardware sampling for analysis.&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>cython</category><category>julia</category><category>Numba</category><category>PyPy</category><category>python</category><guid>https://markdewing.github.io/blog/posts/first-performance-improvements/</guid><pubDate>Fri, 18 Sep 2015 04:48:00 GMT</pubDate></item><item><title>Comparing languages with miniapps</title><link>https://markdewing.github.io/blog/posts/comparing-languages-with-miniapps/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;The &lt;a href="https://mantevo.org/"&gt;Mantevo project&lt;/a&gt; provides a collection of miniapps, which are simplified versions
of real scientific applications that make it easier to explore performance, scaling, languages, programming models, etc.
I want to focus on the language aspect and port some apps to new languages to see how they compare. &lt;/p&gt;
&lt;p&gt;The first miniapp I started with is &lt;a href="http://www.exmatex.org/comd.html"&gt;CoMD&lt;/a&gt;, a molecular dynamics code in C.&lt;/p&gt;
&lt;p&gt;For the language ports, I made multiple variants of increasing complexity.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;nsquared&lt;/dt&gt;
&lt;dd&gt;This uses the naive algorithm in computing inter-particle interactions. The central loop computes the
   interaction of every particle with every other particle.  The scaling of run time vs number of particles is N&lt;sup&gt;2&lt;/sup&gt;.&lt;/dd&gt;
&lt;dt&gt;cell&lt;/dt&gt;
&lt;dd&gt;The cell list method divides space into cells and tracks the particles in each cell.  When computing interactions, only the particles in neighboring cells need to be considered.  The scaling of run time vs. the number of particles is N.&lt;/dd&gt;
&lt;dt&gt;mpi&lt;/dt&gt;
&lt;dd&gt;Parallel version of the cell method.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;The C version corresponds to the 'cell' and 'mpi' variants (plus the C version has OpenMP and several other programming model variants)&lt;/p&gt;
&lt;p&gt;Currently there are Python and Julia ports for the nsquared and cell variants, and a Python version of the mpi variant.
They are available in my 'multitevo' github repository: &lt;a href="https://github.com/markdewing/multitevo"&gt;https://github.com/markdewing/multitevo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Julia version is a pretty straightforward port of the Python version, so it is probably not very idiomatic Julia code.
(I would be happy to take suggestions from the Julia community on how to improve the style and organization)&lt;/p&gt;
&lt;!--
C does not have multidimensional arrays so the 3D cell indices need to be mapped to a linear index of atom coordinates (and
the reverse), and this involves some complicated expressions.
The Julia and Python version preserve this approach, but since they have multidimensional arrays,
it might make the code much simpler to store the atom information directly in a multidimensional array.
--&gt;

&lt;h3&gt;Scaling with system size&lt;/h3&gt;
&lt;p&gt;First let's verify the scaling of the nsquared version vs. the cell list version (using the Julia versions).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Graph of scaling nsquared and cell list method" src="https://markdewing.github.io/blog/2015/scaling.png"&gt;&lt;/p&gt;
&lt;p&gt;As expected, the cell list variant has better scaling at larger system sizes.&lt;/p&gt;
&lt;!--
Data for nsquared variant
natoms time(s)   time_per_atom (us/atom)
256   0.011350  44.335089
500   0.034140  68.280823
864   0.099966 115.700927
1372   0.239239 174.372460
2048   0.509494 248.776288
2916   1.050071 360.106739
4000   1.942931 485.732711
5324   3.327456 624.991804
6912   5.769225 834.667932
--&gt;

&lt;!--
Data for cell method variant
natoms  time(s)   time_per_atom (us/atom)
256   0.045821 178.987427
500   0.049810  99.619442
864   0.142849 165.333940
1372   0.151120 110.145543
2048   0.310324 151.525439
2916   0.344807 118.246625
4000   0.344878  86.219497
5324   0.621829 116.797365
6912   0.626944  90.703648
--&gt;

&lt;h3&gt;Initial performance&lt;/h3&gt;
&lt;p&gt;For a purely computational code such as this, performance matters.
The ultimate goal is near C/Fortran speeds using a higher-level language to express the algorithm.&lt;/p&gt;
&lt;p&gt;Some initial timings (for a system size of 4000 atoms, using the cell variant)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language/Compiler  &lt;/th&gt;
&lt;th align="left"&gt;Version&lt;/th&gt;
&lt;th align="right"&gt;Time/atom (microseconds)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;C - gcc&lt;/td&gt;
&lt;td align="left"&gt;4.8.2&lt;/td&gt;
&lt;td align="right"&gt;2.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julia&lt;/td&gt;
&lt;td align="left"&gt;0.3.11&lt;/td&gt;
&lt;td align="right"&gt;153.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julia&lt;/td&gt;
&lt;td align="left"&gt;0.4.0-dev+6990&lt;/td&gt;
&lt;td align="right"&gt;88.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Python&lt;/td&gt;
&lt;td align="left"&gt;2.7.10 (from Anaconda)  &lt;/td&gt;
&lt;td align="right"&gt;941.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Numba&lt;/td&gt;
&lt;td align="left"&gt;0.20.0&lt;/td&gt;
&lt;td align="right"&gt;789.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pypy&lt;/td&gt;
&lt;td align="left"&gt;2.6.1&lt;/td&gt;
&lt;td align="right"&gt;98.4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;(Hardware is Xeon E5-2630 v3 @ 2.4 Ghz, OS is Ubuntu 12.04)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: These numbers indicate how a particular version of the language and compiler perform on a particular version of this code. The main purpose for these numbers is a baseline to measure future performance improvements.&lt;/p&gt;
&lt;p&gt;I tried &lt;a href="http://www.cosmology.ethz.ch/research/software-lab/HOPE.html"&gt;HOPE&lt;/a&gt;, a Python to C++ JIT compiler.
It require some modifications to the python code, but then failed in compiling the resulting C++ code.
I also tried &lt;a href="http://www.parakeetpython.com/"&gt;Parakeet&lt;/a&gt;.  It failed to translate the Python code, and I did not investigate further.&lt;/p&gt;
&lt;p&gt;It is clear when comparing to C there is quite a bit of room for improvement in the code using the high-level language compilers (Julia, Numba, PyPy).
Whether that needs to come from the modifications to the code, or improvements in the compilers, I don't know yet.&lt;/p&gt;
&lt;p&gt;The only real performance optimization so far has been adding type declarations to the composite types in Julia.
This boosted performance by about 3x. Without the type declarations, the Julia 0.4.0 speed is about 275 us/atom.
First performance lesson: Add type declarations to composite types in Julia.&lt;/p&gt;
&lt;p&gt;Julia and Numba have a number of similarities and so I want to focus on improving the performance of the code
under these two systems in the next few posts.&lt;/p&gt;
&lt;!--
Julia and Numba have similar stages (infer types, convert to LLVM IR, convert to native code), and so I hope to look
at them in parallel going forward.  
--&gt;&lt;/div&gt;</description><category>CoMD</category><category>julia</category><category>Mantevo</category><category>Numba</category><category>PyPy</category><category>python</category><guid>https://markdewing.github.io/blog/posts/comparing-languages-with-miniapps/</guid><pubDate>Wed, 02 Sep 2015 16:18:50 GMT</pubDate></item></channel></rss>