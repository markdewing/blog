<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Journey to the Center of the Computer (cython)</title><link>https://markdewing.github.io/blog/</link><description></description><atom:link href="https://markdewing.github.io/blog/categories/cython.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Thu, 24 Sep 2015 15:49:42 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Why Types Help Performance</title><link>https://markdewing.github.io/blog/posts/why-types-help-performance/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;In &lt;a href="http://markdewing.github.io/blog/posts/first-performance-improvements"&gt;previous&lt;/a&gt; &lt;a href="http://markdewing.github.io/blog/posts/comparing-languages-with-miniapps/"&gt;posts&lt;/a&gt;, we've seen that adding type information can help the performance of the code generated by dynamic language compilers.
The documentation for Cython annotations talks of 'Python interaction', and Numba has 'object' mode and 'nopython' modes.
In post I will look more at what these mean, and how they affect performance.&lt;/p&gt;
&lt;p&gt;To start, consider how values are represented in a computer, such as a simple integer ('int' type in C).
The bare value takes 4 bytes in memory, and no additional information about it is stored,
such as its type or how much space it occupies.
This information is all implicit at run time &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://markdewing.github.io/blog/posts/why-types-help-performance/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;.
That it takes 4-bytes and is interpreted as an integer is determined at compile time.&lt;/p&gt;
&lt;p&gt;In dynamic languages, this extra information can be queried at run-time.
For example:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt; &lt;span class="s"&gt;'int'&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;The value stored in &lt;code&gt;a&lt;/code&gt; has type &lt;code&gt;int&lt;/code&gt;, Python's integer type.
This extra information must be stored somewhere, and languages often solve this by wrapping
the bare value in an object.
This is usually called 'boxing'.
The value plus type information (and any other information) is called a 'boxed type'.
The bare value is called an 'unboxed type' or a 'primitive value'.&lt;/p&gt;
&lt;p&gt;In Java, different types can be explicitly created (&lt;code&gt;int&lt;/code&gt; vs. &lt;code&gt;Integer&lt;/code&gt;), and the programmer
needs to know the differences and tradeoffs.
(See this &lt;a href="http://stackoverflow.com/questions/13055/what-is-boxing-and-unboxing-and-what-are-the-trade-offs"&gt;Stack Overflow question&lt;/a&gt; for more about boxed and primitive types.)&lt;/p&gt;
&lt;p&gt;Python only has boxed values ('everything is an object'). From the interpreter, this means we can
always determine the type of a value.
If we look a layer down, as would be needed to integrate with C, these values are accessed through the Python API.
The base type of any object is PyObject.  For our simple example, integers are stored as PyIntObject.&lt;/p&gt;
&lt;p&gt;For example, consider the following Python code.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;One way to see what calls the interpreter would make is to compile with Cython.
The following C is the result (simplified - reference counting pieces to the Python API are omitted.)&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;PyObject&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nf"&gt;add&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;PyObject&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;pyx_int_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PyInt_FromLong&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;PyObject&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;pyx_int_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PyInt_FromLong&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;PyObject&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;pyx_tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PyNumber_Add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pyx_int_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pyx_int_2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pyx_tmp&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Without type information, Cython basically unrolls the interpreter loop, and makes
a sequence of Python API calls.
The HTML annotation output highlights lines with Python interaction, and can be expanded to show
the generated code.   This gives feedback on where and what types need to be added to avoid
the Python API calls.&lt;/p&gt;
&lt;p&gt;Add some Cython annotations and the example becomes&lt;/p&gt;
&lt;pre class="code literal-block"&gt; &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;
    &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Now the following code is generated&lt;/p&gt;
&lt;pre class="code literal-block"&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;add&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
 &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
 &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;The add of the two integers is done directly (and runs much faster), rather than going through the Python API call.&lt;/p&gt;
&lt;h3&gt;Numba&lt;/h3&gt;
&lt;p&gt;If its type information is insufficient, Numba will call the Python API for every operation.
Since all the operations occur on Python objects, this is called 'object' mode (slow).
With sufficient type information, code can be generated with no calls to the Python API, and hence
the name 'nopython' mode (fast).&lt;/p&gt;
&lt;h3&gt;Julia&lt;/h3&gt;
&lt;p&gt;Julia has boxed object types, but is designed to try use the unboxed types as much as possible.
The most generic type is called 'Any', and it is possible to produce Julia code that runs this mode.&lt;br&gt;
See the section on &lt;a href="http://julia.readthedocs.org/en/latest/manual/embedding/"&gt;Embedding Julia&lt;/a&gt; in the
documentation for more about Julia objects.&lt;/p&gt;
&lt;p&gt;Julia's type inference only happens inside functions.
This is why composite types (structures) need type annotations for good performance.&lt;/p&gt;
&lt;p&gt;This example demonstrates the issue&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;type&lt;/span&gt;&lt;span class="nc"&gt; Example&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="nf"&gt; add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;val&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="n"&gt;v1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Example&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;v2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Example&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="n"&gt;code_llvm&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c"&gt;# The @code_llvm macro prints the LLVM IR.  &lt;/span&gt;
&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"res = "&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Since the type of the 'val' element is not known, the code operates on a generic object type &lt;code&gt;jl_value_t&lt;/code&gt; and eventually calls &lt;code&gt;jl_apply_generic&lt;/code&gt;, which looks up the right method and dispatches to it at execution time.
(The LLVM IR is not shown here - run the example to see it.)  Doing all this at execution time is slow.&lt;/p&gt;
&lt;p&gt;Now add the type annotation&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;type&lt;/span&gt;&lt;span class="nc"&gt; Example&lt;/span&gt;
    &lt;span class="n"&gt;val&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;Int&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;The resulting LLVM IR (also not shown here) is much shorter in that it adds two integers directly and
returns the result as an integer.
With type information, the lookup and dispatch decisions can be made at compile time.&lt;/p&gt;
&lt;p&gt;Note that Julia uses a Just In Time (JIT) compiler, which means compilation occurs at run time.
The run time can be split into various phases, which include compilation and execution of
the resulting code.&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;Hopefully this post sheds some light on how type information can affect the performance of dynamic languages.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;The type can be accessible at run time via debug information.  See this Strange Loop 2014 talk: &lt;a href="https://www.youtube.com/watch?v=LwicN2u6Dro"&gt;Liberating the lurking Smalltalk in Unix&lt;/a&gt; &lt;a class="footnote-backref" href="https://markdewing.github.io/blog/posts/why-types-help-performance/#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>cython</category><category>julia</category><category>Numba</category><category>python</category><guid>https://markdewing.github.io/blog/posts/why-types-help-performance/</guid><pubDate>Thu, 24 Sep 2015 15:48:00 GMT</pubDate></item><item><title>First Performance Improvements</title><link>https://markdewing.github.io/blog/posts/first-performance-improvements/</link><dc:creator>Mark Dewing</dc:creator><description>&lt;div&gt;&lt;p&gt;The &lt;a href="http://markdewing.github.io/blog/posts/comparing-languages-with-miniapps/"&gt;previous post&lt;/a&gt; introduced the CoMD miniapp in Python and Julia.
This post will look a little deeper into the performance of Julia and various Python compilers.
I will use the nsquared version of the code for simplicity.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;computeForce&lt;/code&gt; function in &lt;code&gt;ljforce&lt;/code&gt; takes almost all the time, and it is here we should focus.
In the nsquared version there are no further function calls inside this loop, which should make it
easier to analyze and improve the performance.&lt;/p&gt;
&lt;p&gt;The summary performance table (all times are in microseconds/atom, the system is the smallest size at 256 atoms)&lt;/p&gt;
&lt;!-- from laptop --&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language/compiler  &lt;/th&gt;
&lt;th&gt;version   &lt;/th&gt;
&lt;th align="right"&gt;Initial time&lt;/th&gt;
&lt;th align="right"&gt;  Final time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Python&lt;/td&gt;
&lt;td&gt;2.7.10&lt;/td&gt;
&lt;td align="right"&gt;560&lt;/td&gt;
&lt;td align="right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PyPy&lt;/td&gt;
&lt;td&gt;2.6.1&lt;/td&gt;
&lt;td align="right"&gt;98&lt;/td&gt;
&lt;td align="right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HOPE&lt;/td&gt;
&lt;td&gt;0.4.0&lt;/td&gt;
&lt;td align="right"&gt;8&lt;/td&gt;
&lt;td align="right"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cython&lt;/td&gt;
&lt;td&gt;0.23.1&lt;/td&gt;
&lt;td align="right"&gt;335&lt;/td&gt;
&lt;td align="right"&gt;8.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Numba&lt;/td&gt;
&lt;td&gt;0.21.0&lt;/td&gt;
&lt;td align="right"&gt;450&lt;/td&gt;
&lt;td align="right"&gt;8.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julia&lt;/td&gt;
&lt;td&gt;0.5.0-dev+50&lt;/td&gt;
&lt;td align="right"&gt;44&lt;/td&gt;
&lt;td align="right"&gt;7.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- From desktop 
| Python            | 2.7.10  | 438          |            |
| PyPy              |         |  84          |            |
| HOPE              |         |   7          |            |
| Cython            |         | 329          |  49        |
| Numba             |         | 405          |   7        |
| Julia             |         |  45          |  19        |
--&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;(This table uses a different code version and different hardware, so the numbers are not comparable to the previous post)
&lt;br&gt;
(Hardware is i7-3720QM @ 2.6 Ghz and OS is Ubuntu 14.04)
&lt;br&gt;
The 'Initial Time' column results from the minimal amount of code changes to get the compiler working.
&lt;br&gt;
The 'Final Time' is the time after doing some tuning (for this post, anyway - there's still more work to be done).&lt;/p&gt;
&lt;!--The level of tuning in this post is about adding type information and removing temporary memory allocations.
Further investigation of the generated intermediate and assembly code is yet to be done.
--&gt;

&lt;h3&gt;HOPE&lt;/h3&gt;
&lt;p&gt;The HOPE compiler will compile the nsquared version, provided the branch involving the cutoff is modified to avoid the &lt;code&gt;continue&lt;/code&gt; statement.
The loop that zeros the force also needs a simple name change to the loop variable.
The backend C++ compiler is gcc 4.8.4.  No attempt was made to try different compilers or further optimize the generated code.&lt;/p&gt;
&lt;h3&gt;Numba&lt;/h3&gt;
&lt;p&gt;To use Numba, we need to add &lt;code&gt;import numba&lt;/code&gt; to &lt;code&gt;ljforce.py&lt;/code&gt; and add the &lt;code&gt;@numba.jit&lt;/code&gt; decorator to the &lt;code&gt;computeForce&lt;/code&gt; method.
This gives the time in the initial column (450 μs/atom) , which is about a 20% improvement over the plain Python version.&lt;/p&gt;
&lt;p&gt;Code involving attribute lookups cannot currently be compiled to efficient code, and these lookups occur inside this inner loop.
And the compiler will not hoist attribute lookups outside the loop.
This can be done manually by assigning the attribute to a temporary variable before the loop, and replacing the values in the loop body. 
This transformation enables effective compilation of the loop.&lt;/p&gt;
&lt;p&gt;(Internally Numba performs loop-lifting, where it extracts the loop to a separate function in order to compile the loop.)&lt;/p&gt;
&lt;!--Loop-lifting is a way for Numba to extract a loop into a separate function in order to compile the loop.--&gt;

&lt;p&gt;The beginning of the function now looks like&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    &lt;span class="nd"&gt;@numba.jit&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;computeForce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sim&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c"&gt;# hoist the attribute lookups of these values&lt;/span&gt;
        &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;
        &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;
        &lt;span class="n"&gt;bounds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bounds&lt;/span&gt;
        &lt;span class="n"&gt;nAtoms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nAtoms&lt;/span&gt;
        &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;

        &lt;span class="c"&gt;# rest of original function&lt;/span&gt;
        &lt;span class="c"&gt;# ...&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Now Numba can compile the time consuming loop, and this gives about 9 μs/atom.&lt;/p&gt;
&lt;p&gt;The loop that zeros the force can be slightly improved by either looping over the last dimension
explicitly, or by zeroing the entire array at once.  This change yields the final number in the table (8.3 μs/atom).&lt;/p&gt;
&lt;p&gt;The whole modified file is &lt;a href="https://gist.github.com/markdewing/eb0bf52ea1b71995150a"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Cython&lt;/h3&gt;
&lt;p&gt;The simplest change to enable Cython is to move &lt;code&gt;ljforce.py&lt;/code&gt; to &lt;code&gt;ljforce.pyx&lt;/code&gt;, and add &lt;code&gt;import pyximport; pyximport.install()&lt;/code&gt; to the beginning of &lt;code&gt;simflat.py&lt;/code&gt;.
This initial time (335 μs/atom) gives a 40% improvement over regular python, but there is more performance available.&lt;/p&gt;
&lt;p&gt;The first step is to add some type information.
In order to do this we need to hoist the attribute lookups and assign to temporary variables, as in the Numba version.
At this step, we add types for the same variables as the Numba version.
The beginning of the function looks like this:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;computeForce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="n"&gt;nAtoms&lt;/span&gt;
        &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="n"&gt;double&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;
        &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;
        &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;
        &lt;span class="n"&gt;cdef&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double_t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;bounds&lt;/span&gt;

        &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;
        &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;
        &lt;span class="n"&gt;bounds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bounds&lt;/span&gt;
        &lt;span class="n"&gt;nAtoms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;atoms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nAtoms&lt;/span&gt;
        &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;

        &lt;span class="c"&gt;# rest of original function&lt;/span&gt;
        &lt;span class="c"&gt;# ...&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;The time for this change about 54 μs/atom.&lt;/p&gt;
&lt;p&gt;Cython has a convenient feature that creates an annotated HTML file highlighting lines in the 
original file that may causing a performance issue.  Run &lt;code&gt;cython -a ljforce.pyx&lt;/code&gt; to get the report.
This indicates some more type declarations need to be added.
Adding these types gives about 8.6 μs/atom.   Finally a decorator can be added to remove bounds checks (&lt;code&gt;@cython.boundscheck(False)&lt;/code&gt;) to get the final performance in the table (8.3 μs/atom).&lt;/p&gt;
&lt;p&gt;The whole &lt;code&gt;ljforce.pyx&lt;/code&gt; file is &lt;a href="https://gist.github.com/markdewing/7017e23c883a2bd297cb"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Julia&lt;/h3&gt;
&lt;p&gt;The biggest issue in this code seems to be allocations inside the inner loop.
The memory performance tools can indicate where unexpected allocations are occurring.
One tool is to use a command line option (&lt;code&gt;--track-allocation=user&lt;/code&gt;) to the julia executable.&lt;/p&gt;
&lt;p&gt;One problem is a temporary created inside the loop to hold the results of an array operation (the line that computes &lt;code&gt;dd&lt;/code&gt;).
Moving this allocation out of the loop and setting each element separately improves performance 
to 19 μs/atom.
Another allocation occurs when updating the force array using a slice.  Changing this to explicitly loop
over the elements improves the performance to the final numbers shown in the table (7.1 μs/atom).&lt;/p&gt;
&lt;p&gt;The final modified file is &lt;a href="https://gist.github.com/markdewing/f1c9a46a2fec8a3ade4f"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;The performance numbers have quite a bit of variance, and they are not a result of a rigorous benchmarking and statistics collection.  If you want to compare between compilers, the final results should probably be read something like: "The performance of Cython and Numba is roughly the same on this code, and Julia is a little bit faster for this code".
Also keep in mind we're not done yet digging into the performance of these different compilers.&lt;/p&gt;
&lt;p&gt;Some simple changes to the code can give dramatic performance improvements, but the difficulty is
discovering what changes need to be made and where to make them.&lt;/p&gt;
&lt;p&gt;Future topics to explore:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply these lessons to the cell version of the code.&lt;/li&gt;
&lt;li&gt;With Julia and Numba, it's hard to connect intermediate code stages (internal IR, LLVM IR, assembly) to the original code, and to spot potential performance issues there.  The Cython annotation output is nice here.&lt;/li&gt;
&lt;li&gt;The difference between operating on dynamic objects versus the underlying value types.&lt;/li&gt;
&lt;li&gt;How well does the final assembly utilize the hardware. How to use hardware sampling for analysis.&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>cython</category><category>julia</category><category>Numba</category><category>PyPy</category><category>python</category><guid>https://markdewing.github.io/blog/posts/first-performance-improvements/</guid><pubDate>Fri, 18 Sep 2015 04:48:00 GMT</pubDate></item></channel></rss>